<html lang="en" data-bs-theme="dark"><head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TDS 2025 Jan GA3 - Large Language Models</title>
  <link rel="icon" type="image/svg+xml" href="favicon.svg">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css" integrity="sha384-XGjxtQfXaH2tnPFa9x+ruJTuLE3Aa6LhHSWRr1XeTyhezb4abCG4ccI5AkVDxqC+" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.10.0/build/styles/default.min.css" integrity="sha384-4Y0nObtF3CbKnh+lpzmAVdAMtQXl+ganWiiv73RcGVdRdfVIya8Cao1C8ZsVRRDz" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.10.0/build/styles/github-dark.min.css">
  <script src="https://browser.sentry-cdn.com/8.55.0/bundle.tracing.replay.min.js" crossorigin="anonymous"></script><script src="https://js.sentry-cdn.com/f9b738b8a1bfb42581e666ab0a7dbb22.min.js" crossorigin="anonymous"></script>
  <style>
    body {
      padding-top: 64px;
    }

    body[data-status="admin"] #navbar {
      background-color: var(--bs-primary) !important;

      .navbar-brand::before {
        content: "[Admin] ";
      }
    }

    body[data-status="running"] #navbar {
      background-color: var(--bs-success) !important;
    }

    body[data-status="ended"] #navbar {
      background-color: var(--bs-danger) !important;
    }

    [id] {
      scroll-margin-top: 4rem;
    }

    textarea::-webkit-scrollbar {
      width: 8px;
    }

    textarea::-webkit-scrollbar-track {
      background: #333;
    }

    textarea::-webkit-scrollbar-thumb {
      background: #666;
      cursor: default;
    }

    textarea::-webkit-scrollbar-thumb:hover {
      background: #888;
    }

    /* Add a YouTube icon on top of all YouTube links with images */
    a[href*="youtu"]:has(img[src*="ytimg"]) {
      position: relative;
      display: inline-block;
    }
    a[href*="youtu"]:has(img[src*="ytimg"])::after {
      content: "";
      position: absolute;
      left: 50%;
      top: 50%;
      width: 100px;
      height: 100px;
      background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="red" class="bi bi-youtube" viewBox="0 0 16 16"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2z"/><path d="M6.4 5.209v4.818l4.157-2.408z" fill="white"/></svg>') no-repeat center center;
      background-size: contain;
      transform: translate(-50%, -50%);
      pointer-events: none;
    }
</style>
<style id="googleidentityservice_button_styles">.qJTHM{-webkit-user-select:none;color:#202124;direction:ltr;-webkit-touch-callout:none;font-family:"Roboto-Regular",arial,sans-serif;-webkit-font-smoothing:antialiased;font-weight:400;margin:0;overflow:hidden;-webkit-text-size-adjust:100%}.ynRLnc{left:-9999px;position:absolute;top:-9999px}.L6cTce{display:none}.bltWBb{word-break:break-all}.hSRGPd{color:#1a73e8;cursor:pointer;font-weight:500;text-decoration:none}.Bz112c-W3lGp{height:16px;width:16px}.Bz112c-E3DyYd{height:20px;width:20px}.Bz112c-r9oPif{height:24px;width:24px}.Bz112c-uaxL4e{-webkit-border-radius:10px;border-radius:10px}.LgbsSe-Bz112c{display:block}.S9gUrf-YoZ4jf,.S9gUrf-YoZ4jf *{border:none;margin:0;padding:0}.fFW7wc-ibnC6b>.aZ2wEe>div{border-color:#4285f4}.P1ekSe-ZMv3u>div:nth-child(1){background-color:#1a73e8!important}.P1ekSe-ZMv3u>div:nth-child(2),.P1ekSe-ZMv3u>div:nth-child(3){background-image:linear-gradient(to right,rgba(255,255,255,.7),rgba(255,255,255,.7)),linear-gradient(to right,#1a73e8,#1a73e8)!important}.haAclf{display:inline-block}.nsm7Bb-HzV7m-LgbsSe{-webkit-border-radius:4px;border-radius:4px;-webkit-box-sizing:border-box;box-sizing:border-box;-webkit-transition:background-color .218s,border-color .218s;transition:background-color .218s,border-color .218s;-webkit-user-select:none;-webkit-appearance:none;background-color:#fff;background-image:none;border:1px solid #dadce0;color:#3c4043;cursor:pointer;font-family:"Google Sans",arial,sans-serif;font-size:14px;height:40px;letter-spacing:0.25px;outline:none;overflow:hidden;padding:0 12px;position:relative;text-align:center;vertical-align:middle;white-space:nowrap;width:auto}@media screen and (-ms-high-contrast:active){.nsm7Bb-HzV7m-LgbsSe{border:2px solid windowText;color:windowText}}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe{font-size:14px;height:32px;letter-spacing:0.25px;padding:0 10px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe{font-size:11px;height:20px;letter-spacing:0.3px;padding:0 8px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe{padding:0;width:40px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.pSzOP-SxQuSe{width:32px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.purZT-SxQuSe{width:20px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK{-webkit-border-radius:20px;border-radius:20px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK.pSzOP-SxQuSe{-webkit-border-radius:16px;border-radius:16px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK.purZT-SxQuSe{-webkit-border-radius:10px;border-radius:10px}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc{border:none;color:#fff}.nsm7Bb-HzV7m-LgbsSe.MFS4be-v3pZbf-Ia7Qfc{background-color:#1a73e8}.nsm7Bb-HzV7m-LgbsSe.MFS4be-JaPV2b-Ia7Qfc{background-color:#202124;color:#e8eaed}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:18px;margin-right:8px;min-width:18px;width:18px}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:14px;min-width:14px;width:14px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:10px;min-width:10px;width:10px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin-left:8px;margin-right:-4px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin:0;padding:10px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{padding:8px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{padding:4px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-top-left-radius:3px;border-top-left-radius:3px;-webkit-border-bottom-left-radius:3px;border-bottom-left-radius:3px;display:-webkit-box;display:-webkit-flex;display:flex;justify-content:center;-webkit-align-items:center;align-items:center;background-color:#fff;height:36px;margin-left:-10px;margin-right:12px;min-width:36px;width:36px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf .nsm7Bb-HzV7m-LgbsSe-Bz112c,.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin:0;padding:0}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{height:28px;margin-left:-8px;margin-right:10px;min-width:28px;width:28px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{height:16px;margin-left:-6px;margin-right:8px;min-width:16px;width:16px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:3px;border-radius:3px;margin-left:2px;margin-right:0;padding:0}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:18px;border-radius:18px}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:14px;border-radius:14px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:8px;border-radius:8px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-bN97Pc-sM5MNb{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-align-items:center;align-items:center;-webkit-flex-direction:row;flex-direction:row;justify-content:space-between;-webkit-flex-wrap:nowrap;flex-wrap:nowrap;height:100%;position:relative;width:100%}.nsm7Bb-HzV7m-LgbsSe .oXtfBe-l4eHX{justify-content:center}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-BPrWId{-webkit-flex-grow:1;flex-grow:1;font-family:"Google Sans",arial,sans-serif;font-weight:500;overflow:hidden;text-overflow:ellipsis;vertical-align:top}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-BPrWId{font-weight:300}.nsm7Bb-HzV7m-LgbsSe .oXtfBe-l4eHX .nsm7Bb-HzV7m-LgbsSe-BPrWId{-webkit-flex-grow:0;flex-grow:0}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-MJoBVe{-webkit-transition:background-color .218s;transition:background-color .218s;bottom:0;left:0;position:absolute;right:0;top:0}.nsm7Bb-HzV7m-LgbsSe:hover,.nsm7Bb-HzV7m-LgbsSe:focus{-webkit-box-shadow:none;box-shadow:none;border-color:rgb(210,227,252);outline:none}.nsm7Bb-HzV7m-LgbsSe:hover .nsm7Bb-HzV7m-LgbsSe-MJoBVe,.nsm7Bb-HzV7m-LgbsSe:focus .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(66,133,244,.04)}.nsm7Bb-HzV7m-LgbsSe:active .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(66,133,244,.1)}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:hover .nsm7Bb-HzV7m-LgbsSe-MJoBVe,.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:focus .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(255,255,255,.24)}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:active .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(255,255,255,.32)}.nsm7Bb-HzV7m-LgbsSe .n1UuX-DkfjY{-webkit-border-radius:50%;border-radius:50%;display:-webkit-box;display:-webkit-flex;display:flex;height:20px;margin-left:-4px;margin-right:8px;min-width:20px;width:20px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId{font-family:"Roboto";font-size:12px;text-align:left}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .ssJRIf,.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff .fmcmS{overflow:hidden;text-overflow:ellipsis}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-align-items:center;align-items:center;color:#5f6368;fill:#5f6368;font-size:11px;font-weight:400}.nsm7Bb-HzV7m-LgbsSe.jVeSEe.MFS4be-Ia7Qfc .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff{color:#e8eaed;fill:#e8eaed}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff .Bz112c{height:18px;margin:-3px -3px -3px 2px;min-width:18px;width:18px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-top-left-radius:0;border-top-left-radius:0;-webkit-border-bottom-left-radius:0;border-bottom-left-radius:0;-webkit-border-top-right-radius:3px;border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;border-bottom-right-radius:3px;margin-left:12px;margin-right:-10px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:18px;border-radius:18px}.L5Fo6c-sM5MNb{border:0;display:block;left:0;position:relative;top:0}.L5Fo6c-bF1uUb{-webkit-border-radius:4px;border-radius:4px;bottom:0;cursor:pointer;left:0;position:absolute;right:0;top:0}.L5Fo6c-bF1uUb:focus{border:none;outline:none}sentinel{}</style></head>

<body data-status="ended">

  <nav class="navbar navbar-expand-lg fixed-top bg-body-tertiary" data-bs-theme="dark" id="navbar">
    <div class="container-fluid">
      <span class="navbar-brand" href=".">
        <span id="countdown">Ended at Wed, 5 Feb, 2025, 6:29 pm GMT</span>
        <button id="score" type="button" class="btn btn-dark btn-sm mb-1 ms-2" disabled="">Score: 0</button>
        <button type="button" class="btn btn-outline-light btn-sm mb-1 ms-2 check-action" title="Check your score" disabled="">Check all</button>
        <button type="button" class="btn btn-outline-light btn-sm mb-1 ms-2 save-action" title="Save your progress" disabled="">Save</button>
      </span>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="nav-item dropdown ms-auto" role="group" aria-label="Toggle dark mode" title="Toggle Dark Mode">
          <button class="dark-theme-toggle btn btn-outline-light dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Toggle theme (dark)" fdprocessedid="470dk">
            <i class="bi bi-circle-half"></i> <span class="d-lg-none ms-2">Toggle theme</span>
          </button>
          <ul class="dropdown-menu dropdown-menu-end">
            <li><button class="dropdown-item" data-bs-theme-value="light" aria-pressed="false"><i class="me-2 bi bi-sun-fill"></i> Light</button></li>
            <li><button class="dropdown-item active" data-bs-theme-value="dark" aria-pressed="true"><i class="me-2 bi bi-moon-stars-fill"></i> Dark</button></li>
            <li><button class="dropdown-item" data-bs-theme-value="auto" aria-pressed="false"><i class="me-2 bi bi-circle-half"></i> Auto</button></li>
          </ul>
        </div>
      </div>
    </div>
  </nav>

  <main class="container">
    <section id="instructions">
    <h1 class="display-3 my-5">
      TDS 2025 Jan GA3 - Large Language Models
    </h1>

    <h2 class="display-6 my-5">Instructions</h2>
    <ol>
      <li><strong>Learn what you need</strong>. Reading material is provided, but feel free to skip it if you can answer the question. (Or learn it, just for pleasure.)</li>
      <li><strong>Check answers regularly</strong> by pressing <kbd>Check</kbd>. It shows which answers are right or wrong. You can check multiple times.</li>
      <li><strong>Save regularly</strong> by pressing <kbd>Save</kbd>. You can save multiple times. Your last saved submission will be evaluated.</li>
      <li><strong>Reloading is OK</strong>. Your answers are saved in your browser (not server). Questions won't change except for randomized parameters.</li>
      <li><strong>Browser may struggle</strong>. If you face loading issues, turn off security restrictions or try a different browser.</li>
      <li><strong>Use anything</strong>. You can use any resources you want. The Internet, ChatGPT, friends, whatever. Use any libraries or frameworks you want.</li>
      <li><strong>It's hackable</strong>. It's possible to get the answer to <em>some</em> questions by hacking the code for this quiz. That's allowed.</li>
    </ol>
    <p><strong>Note:</strong> You'll run multiple servers in this exam. <em>All of them</em> must be running simultaneously while checking or saving answers.</p>
    <div class="alert alert-info d-flex align-items-center p-4 border-start border-info border-2 mt-5">
      <i class="bi bi-chat-square-text-fill fs-4 me-3"></i>
      <div>
        <strong>Have questions?</strong>
        <a href="https://discourse.onlinedegree.iitm.ac.in/t/ga3-large-language-models-discussion-thread-tds-jan-2025/163247" target="_blank" class="alert-link ms-2">
          Join the discussion on Discourse
        </a>
      </div>
    </div>
  </section>

    <section id="login" class="my-5"><!---->
        <div id="logged-in-email" class="mb-3">You are logged in as <strong><!--?lit$697713494$-->23ds3000149@ds.study.iitm.ac.in</strong>.</div>
        <div class="mb-3">
          <button class="btn btn-sm btn-outline-danger" fdprocessedid="kwyyzj">
            Logout
          </button>
        </div>
      </section>

    <section id="notification" class="my-5"><div class="alert alert-success" role="alert">
        <h4 class="alert-heading">Recent saves <small class="text-muted fs-6 fw-light">(most recent is your official score)</small></h4>
        <div class="d-flex align-items-center mt-2">
  <button class="btn btn-sm btn-outline-primary me-2 load-answers" data-bs-toggle="collapse" data-index="0" fdprocessedid="lx49mn">Reload</button>
  from 05/02/2025, 18:05:09. Score: 7
</div><div class="d-flex align-items-center mt-2">
  <button class="btn btn-sm btn-outline-primary me-2 load-answers" data-bs-toggle="collapse" data-index="1" fdprocessedid="6kefp9">Reload</button>
  from 05/02/2025, 01:11:59. Score: 7
</div><div class="d-flex align-items-center mt-2">
  <button class="btn btn-sm btn-outline-primary me-2 load-answers" data-bs-toggle="collapse" data-index="2" fdprocessedid="b84dpk">Reload</button>
  from 05/02/2025, 00:51:04. Score: 7
</div>
        </div></section>

    <section class="alert alert-info my-5 d-flex align-items-center d-none" id="loading-questions">
      <div class="spinner-border text-primary me-2" role="status"></div>
      <span>Loading questions...</span>
    </section>

    <form id="exam-form" class="needs-validation" novalidate="">
      <section id="questions" class="my-5"><!----><!----><h1 class="display-6">Questions</h1><!----><!----><ol class="mt-3">
        <!--?lit$697713494$--><!----><li><a href="#hq-llm-sentiment-analysis"><!--?lit$697713494$-->LLM Sentiment Analysis</a> (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)</li><!----><!----><li><a href="#hq-token-cost"><!--?lit$697713494$-->LLM Token Cost</a> (<!--?lit$697713494$-->0.75 <!--?lit$697713494$-->marks)</li><!----><!----><li><a href="#hq-generate-addresses-with-llms"><!--?lit$697713494$-->Generate addresses with LLMs</a> (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)</li><!----><!----><li><a href="#hq-llm-vision"><!--?lit$697713494$-->LLM Vision</a> (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)</li><!----><!----><li><a href="#hq-llm-embeddings"><!--?lit$697713494$-->LLM Embeddings</a> (<!--?lit$697713494$-->0.75 <!--?lit$697713494$-->marks)</li><!----><!----><li><a href="#hq-embedding-similarity"><!--?lit$697713494$-->Embedding Similarity</a> (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)</li><!----><!----><li><a href="#hq-vector-databases"><!--?lit$697713494$-->Vector Databases</a> (<!--?lit$697713494$-->1.5 <!--?lit$697713494$-->marks)</li><!----><!----><li><a href="#hq-function-calling"><!--?lit$697713494$-->Function Calling</a> (<!--?lit$697713494$-->1.5 <!--?lit$697713494$-->marks)</li><!----><!----><li><a href="#hq-get-llm-to-say-yes"><!--?lit$697713494$-->Get an LLM to say Yes</a> (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)</li><!---->
      </ol><!----><!---->
          <div class="card my-5" data-question="q-llm-sentiment-analysis" id="hq-llm-sentiment-analysis">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->1</span>
              <!--?lit$697713494$-->LLM Sentiment Analysis (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>LLM Sentiment Analysis</h2>
<p><a href="https://platform.openai.com/" target="_blank" rel="noopener noreferrer">OpenAI's API</a> provides access to language models like GPT 4o, GPT 4o mini, etc.</p>
<p>For more details, read OpenAI's guide for:</p>
<ul>
<li><a href="https://platform.openai.com/docs/guides/text-generation" target="_blank" rel="noopener noreferrer">Text Generation</a></li>
<li><a href="https://platform.openai.com/docs/guides/vision" target="_blank" rel="noopener noreferrer">Vision</a></li>
<li><a href="https://platform.openai.com/docs/guides/structured-outputs" target="_blank" rel="noopener noreferrer">Structured Outputs</a></li>
</ul>
<p>Start with this quick tutorial:</p>
<p><a href="https://youtu.be/Xz4ORA0cOwQ" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/Xz4ORA0cOwQ/sddefault.webp" alt="OpenAI API Quickstart: Send your First API Request" class="img-fluid"></a></p>
<p>Here's a minimal example using <code>curl</code> to generate text:</p>
<pre><code class="language-bash hljs" data-highlighted="yes">curl https://api.openai.com/v1/chat/completions \
  -H <span class="hljs-string">"Content-Type: application/json"</span> \
  -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$OPENAI_API_KEY</span>"</span> \
  -d <span class="hljs-string">'{
    "model": "gpt-4o-mini",
    "messages": [{ "role": "user", "content": "Write a haiku about programming." }]
  }'</span>
</code></pre>
<p>Let's break down the request:</p>
<ul>
<li><code>curl https://api.openai.com/v1/chat/completions</code>: The API endpoint for text generation.</li>
<li><code>-H "Content-Type: application/json"</code>: The content type of the request.</li>
<li><code>-H "Authorization: Bearer $OPENAI_API_KEY"</code>: The API key for authentication.</li>
<li><code>-d</code>: The request body.<ul>
<li><code>"model": "gpt-4o-mini"</code>: The model to use for text generation.</li>
<li><code>"messages":</code>: The messages to send to the model.<ul>
<li><code>"role": "user"</code>: The role of the message.</li>
<li><code>"content": "Write a haiku about programming."</code>: The content of the message.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="https://youtu.be/_D46QrX-2iU" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/_D46QrX-2iU/sddefault.webp" alt="LLM Sentiment Analysis" class="img-fluid"></a></p>
<p>This video explains how to use large language models (LLMs) for sentiment analysis and classification, covering:</p>
<ul>
<li><strong>Sentiment Analysis</strong>: Use OpenAI API to identify the sentiment of movie reviews as positive or negative.</li>
<li><strong>Prompt Engineering</strong>: Learn how to craft effective prompts to get desired results from LLMs.</li>
<li><strong>LLM Training</strong>: Understand how to train LLMs by providing examples and feedback.</li>
<li><strong>OpenAI API Integration</strong>: Integrate OpenAI API into Python code to perform sentiment analysis.</li>
<li><strong>Tokenization</strong>: Learn about tokenization and its impact on LLM input and cost.</li>
<li><strong>Zero-Shot, One-Shot, and Multi-Shot Learning</strong>: Understand different approaches to using LLMs for learning.</li>
</ul>
<p>Here are the links used in the video:</p>
<ul>
<li><a href="https://colab.research.google.com/drive/1tVZBD9PKto1kPmVJFNUt0tdzT5EmLLWs" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a></li>
<li><a href="https://drive.google.com/file/d/1X33ao8_PE17c3htkQ-1p2dmW2xKmOq8Q/view" target="_blank" rel="noopener noreferrer">Movie reviews dataset</a></li>
<li><a href="https://platform.openai.com/playground/chat" target="_blank" rel="noopener noreferrer">OpenAI Playground</a></li>
<li><a href="https://openai.com/api/pricing/" target="_blank" rel="noopener noreferrer">OpenAI Pricing</a></li>
<li><a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">OpenAI Tokenizer</a></li>
<li><a href="https://platform.openai.com/docs/api-reference/" target="_blank" rel="noopener noreferrer">OpenAI API Reference</a></li>
<li><a href="https://platform.openai.com/docs/overview" target="_blank" rel="noopener noreferrer">OpenAI Docs</a></li>
</ul>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <p>
      DataSentinel Inc. is a tech company specializing in building advanced natural language processing (NLP) solutions.
      Their latest project involves integrating an AI-powered sentiment analysis module into an internal monitoring
      dashboard. The goal is to automatically classify large volumes of unstructured feedback and text data from various
      sources as either GOOD, BAD, or NEUTRAL. As part of the quality assurance process, the development team needs to
      test the integration with a series of sample inputs—even ones that may not represent coherent text—to ensure that
      the system routes and processes the data correctly.
    </p>
    <p>
      Before rolling out the live system, the team creates a test harness using Python. The harness employs the
      <code>httpx</code> library to send POST requests to OpenAI's API. For this proof-of-concept, the team uses the
      dummy model <code>gpt-4o-mini</code> along with a dummy API key in the Authorization header to simulate real API
      calls.
    </p>
    <p>One of the test cases involves sending a sample piece of meaningless text:</p>
    <pre><code data-highlighted="yes" class="hljs language-nginx"><span class="hljs-attribute">q8Zl</span>
rnNGdi IXTIMdv Xd1 vP1 ZWF xMsEHp2LUiOAw7E1</code></pre>
    <p>
      Write a Python program that uses <code>httpx</code> to send a POST request to OpenAI's API to analyze the
      sentiment of this (meaningless) text into GOOD, BAD or NEUTRAL. Specifically:
    </p>
    <ol>
      <li>Make sure you pass an Authorization header with dummy API key.</li>
      <li>Use <code>gpt-4o-mini</code> as the model.</li>
      <li>
        The first message must be a system message asking the LLM to analyze the sentiment of the text. Make sure you
        mention GOOD, BAD, or NEUTRAL as the categories.
      </li>
      <li>The second message must be <strong>exactly</strong> the text contained above.</li>
    </ol>
    <p>
      This test is crucial for DataSentinel Inc. as it validates both the API integration and the correctness of message
      formatting in a controlled environment. Once verified, the same mechanism will be used to process genuine customer
      feedback, ensuring that the sentiment analysis module reliably categorizes data as GOOD, BAD, or NEUTRAL. This
      reliability is essential for maintaining high operational standards and swift response times in real-world
      applications.
    </p>
    <p><strong>Note</strong>: This uses a dummy <code>httpx</code> library, not the real one. You can only use:</p>
    <ol>
      <li><code class="language-python">response = httpx.get(url, **kwargs)</code></li>
      <li><code class="language-python">response = httpx.post(url, json=None, **kwargs)</code></li>
      <li><code class="language-python">response.raise_for_status()</code></li>
      <li><code class="language-python">response.json()</code></li>
    </ol>
    <div class="mb-3">
      <label class="form-label" for="q-llm-sentiment-analysis">Code</label>
      <textarea class="form-control font-monospace text-bg-dark" rows="6" id="q-llm-sentiment-analysis" name="q-llm-sentiment-analysis" disabled=""></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-llm-sentiment-analysis" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-token-cost" id="hq-token-cost">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->2</span>
              <!--?lit$697713494$-->LLM Token Cost (<!--?lit$697713494$-->0.75 <!--?lit$697713494$-->marks)
            </div>
            <!--?lit$697713494$-->
            <div class="card-body"><!--?lit$697713494$-->
    <p>
      LexiSolve Inc. is a startup that delivers a conversational AI platform to enterprise clients. The system leverages
      OpenAI’s language models to power a variety of customer service, sentiment analysis, and data extraction features.
      Because pricing for these models is based on the number of tokens processed—and strict token limits apply—accurate
      token accounting is critical for managing costs and ensuring system stability.
    </p>
    <p>
      To optimize operational costs and prevent unexpected API overages, the engineering team at LexiSolve has developed
      an internal diagnostic tool that simulates and measures token usage for typical prompts sent to the language
      model.
    </p>
    <p>
      One specific test case an understanding of text tokenization. Your task is to generate data for that test case.
    </p>
    <p>Specifically, when you make a request to OpenAI's GPT-4o-Mini with <em>just this user message</em>:</p>
    <pre style="white-space: pre-wrap;"><code class="language-text hljs language-plaintext" data-highlighted="yes">List only the valid English words from these: IgwsJe6A, EnDFTY, g, JOPp6dLM7, kY, z9eQ0, kqvIovXz, 6xCz6WN9, Qy, ue, Hkk, P, idv8VX, IYkm1IdgxZ, XLVkWpJ, PA8, TnPEW, fLWKD, zC7yHUc, uOewAPoX, o6, 2, bbWp, h, e5H, yMeSfoqI, lNgOfG, eFHQox7, fvThW4HV2f, X3Zq, Hij, i41CY, HQO9YlmP, 6zxf3huQ, z, 2NKhmE8C, h, 4n, rXm1M6LR, 7eyC7UKj, pxRMAP, nZZBRQGaRc, 3QdpgnH9, Cy, lA, moDDK, OmBQ, dDjhMZ, Ea5BHrNyg, 3uskK, av, Oobg, R1G, IOE6H, 7, cu, d, j, LcOxAfW93w, 5KYvzXc6, sa2h, EJZu, YU, exA2g, BBZWs, stQeMiOPk, Q, gZv94Ct6X4, KeS, k2oRFhR, fluQT, ZAmFk6Q, 2wp5bU, vJj, ALZAGzUp, 4jZEGZns, lBdECVPn, RObYbgNBh, nFnC19P, WI4C8o9, tuNQWDdDN</code></pre>
    <p>... how many <em>input tokens</em> does it use up?</p>

    <div class="mb-3">
      <label class="form-label" for="q-token-cost">Number of tokens:</label>
      <input class="form-control" id="q-token-cost" name="q-token-cost" disabled="" fdprocessedid="iutuai"><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      <p class="text-muted">
        Remember: indicating that this is a <code>user</code> message takes up a few extra tokens. You actually need to
        make the request to get the answer.
      </p>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-token-cost" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-generate-addresses-with-llms" id="hq-generate-addresses-with-llms">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->3</span>
              <!--?lit$697713494$-->Generate addresses with LLMs (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>LLM Text Extraction</h2>
<p><a href="json.md" target="_blank" rel="noopener noreferrer">JSON</a> is one of the most widely used formats in the world for applications to exchange data.</p>
<p><a href="https://youtu.be/72514uGffPE" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/72514uGffPE/sddefault.webp" alt="LLM Extraction" class="img-fluid"></a></p>
<p>This video explains how to use LLMs to extract structure from unstructured data, covering:</p>
<ul>
<li><strong>LLM for Data Extraction</strong>: Use OpenAI's API to extract structured information from unstructured data like addresses.</li>
<li><strong>JSON Schema</strong>: Define a JSON schema to ensure consistent and structured output from the LLM.</li>
<li><strong>Prompt Engineering</strong>: Craft effective prompts to guide the LLM's response and improve accuracy.</li>
<li><strong>Data Cleaning</strong>: Use string functions and OpenAI's API to clean and standardize data.</li>
<li><strong>Data Analysis</strong>: Analyze extracted data using Pandas to gain insights.</li>
<li><strong>LLM Limitations</strong>: Understand the limitations of LLMs, including potential errors and inconsistencies in output.</li>
<li><strong>Production Use Cases</strong>: Explore real-world applications of LLMs for data extraction, such as customer service email analysis.</li>
</ul>
<p>Here are the links used in the video:</p>
<ul>
<li><a href="https://colab.research.google.com/drive/1Z8mG-RPTSYY4qwkoNdzRTc4StbnwOXeE" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a></li>
<li><a href="https://json-schema.org/" target="_blank" rel="noopener noreferrer">JSON Schema</a></li>
<li><a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener noreferrer">Function calling</a></li>
</ul>
<p>Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied
<a href="https://json-schema.org/overview/what-is-jsonschema" target="_blank" rel="noopener noreferrer">JSON Schema</a>, so you don't need to worry about the model omitting a required key,
or hallucinating an invalid enum value.</p>
<pre><code class="language-bash hljs" data-highlighted="yes">curl https://api.openai.com/v1/chat/completions \
-H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$OPENAI_API_KEY</span>"</span> \
-H <span class="hljs-string">"Content-Type: application/json"</span> \
-d <span class="hljs-string">'{
  "model": "gpt-4o-2024-08-06",
  "messages": [
    { "role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step." },
    { "role": "user", "content": "how can I solve 8x + 7 = -23" }
  ],
  "response_format": {
    "type": "json_schema",
    "json_schema": {
      "name": "math_response",
      "strict": true
      "schema": {
        "type": "object",
        "properties": {
          "steps": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": { "explanation": { "type": "string" }, "output": { "type": "string" } },
              "required": ["explanation", "output"],
              "additionalProperties": false
            }
          },
          "final_answer": { "type": "string" }
        },
        "required": ["steps", "final_answer"],
        "additionalProperties": false
      }
    }
  }
}'</span>
</code></pre>
<p>Here's what the <code>response_format</code> tells OpenAI. The items marked ⚠️ are OpenAI specific requirements for the JSON schema.</p>
<ul>
<li><code>"type": "json_schema"</code>: We want you to generate a JSON response that follows this schema.</li>
<li><code>"json_schema":</code>: We're going to give you a schema.<ul>
<li><code>"name": "math_response"</code>: The schema is called <code>math_response</code>. (We can call it anything.)</li>
<li><code>"strict": true</code>: Follow the schema exactly.</li>
<li><code>"schema":</code>: Now, here's the actual JSON schema.<ul>
<li><code>"type": "object"</code>: Return an object. ⚠️ The root object <strong>must</strong> be an object.</li>
<li><code>"properties":</code>: The object has these properties:<ul>
<li><code>"steps":</code>: There's a <code>steps</code> property.<ul>
<li><code>"type": "array"</code>: It's an array.</li>
<li><code>"items":</code>: Each item in the array...<ul>
<li><code>"type": "object"</code>: ... is an object.</li>
<li><code>"properties":</code>: The object has these properties:<ul>
<li><code>"explanation":</code>: There's an <code>explanation</code> property.<ul>
<li><code>"type": "string"</code>: ... which is a string.</li>
</ul>
</li>
<li><code>"output":</code>: There's an <code>output</code> property.<ul>
<li><code>"type": "string"</code>: ... which is a string, too.</li>
</ul>
</li>
</ul>
</li>
<li><code>"required": ["explanation", "output"]</code>: ⚠️ You <strong>must</strong> add <code>"required": [...]</code> and include <strong>all</strong> fields int he object.</li>
<li><code>"additionalProperties": false</code>: ⚠️ OpenAI requires every object to have <code>"additionalProperties": false</code>.</li>
</ul>
</li>
</ul>
</li>
<li><code>"final_answer":</code>: There's a <code>final_answer</code> property.<ul>
<li><code>"type": "string"</code>: ... which is a string.</li>
</ul>
</li>
</ul>
</li>
<li><code>"required": ["steps", "final_answer"]</code>: ⚠️ You <strong>must</strong> add <code>"required": [...]</code> and include <strong>all</strong> fields in the object.</li>
<li><code>"additionalProperties": false</code>: ⚠️ OpenAI requires every object to have <code>"additionalProperties": false</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <style scoped="">
      textarea[name="<!---->q-generate-addresses-with-llms"] {
        opacity: 0;
        pointer-events: none;
      }
    <!----></style>
    <p>
      RapidRoute Solutions is a logistics and delivery company that relies on accurate and standardized address data to
      optimize package routing. Recently, they encountered challenges with manually collecting and verifying new
      addresses for testing their planning software. To overcome this, the company decided to create an automated
      address generator using a language model, which would provide realistic, standardized U.S. addresses that could be
      directly integrated into their system.
    </p>
    <p>
      The engineering team at RapidRoute is tasked with designing a service that uses OpenAI's GPT-4o-Mini model to
      generate fake but plausible address data. The addresses must follow a strict format, which is critical for
      downstream processes such as geocoding, routing, and verification against customer databases. For consistency and
      validation, the development team requires that the addresses be returned as structured JSON data with no
      additional properties that could confuse their parsers.
    </p>

    <p>
      As part of the integration process, you need to write the body of the request to an
      <a href="https://platform.openai.com/docs/api-reference/chat">OpenAI chat completion call</a> that:
    </p>
    <ul>
      <li>Uses model <code>gpt-4o-mini</code></li>
      <li>Has a <code>system</code> message: <code>Respond in JSON</code></li>
      <li>Has a user message: <code>Generate 10 random addresses in the US</code></li>
      <li>
        Uses <a href="https://platform.openai.com/docs/guides/structured-outputs/">structured outputs</a> to respond
        with an object <code>addresses</code> which is an array of objects with <strong>required</strong> fields:
        <!--?lit$697713494$--><!----><code><!--?lit$697713494$-->zip</code> (<!--?lit$697713494$-->number) <!----><!----><code><!--?lit$697713494$-->longitude</code> (<!--?lit$697713494$-->number) <!----><!----><code><!--?lit$697713494$-->city</code> (<!--?lit$697713494$-->string) <!---->.
      </li>
      <li>Sets <code>additionalProperties</code> to <code>false</code> to prevent additional properties.</li>
    </ul>
    <p>
      Note that you don't need to run the request or use an API key; your task is simply to write the correct JSON body.
    </p>
    <div class="mb-3">
      <label class="form-label" for="q-generate-addresses-with-llms">What is the JSON body we should send to <code>https://api.openai.com/v1/chat/completions</code> for this? (No
        need to run it or to use an API key. Just write the body of the request below.)</label>
      <textarea class="form-control font-monospace text-bg-dark d-none" rows="6" disabled="" id="q-generate-addresses-with-llms" name="q-generate-addresses-with-llms"></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      <p class="text-muted">There's no answer box above. Figure out how to enable it. That's part of the test.</p>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-generate-addresses-with-llms" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-llm-vision" id="hq-llm-vision">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->4</span>
              <!--?lit$697713494$-->LLM Vision (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h1>Base 64 Encoding</h1>
<p>Base64 is a method to convert binary data into ASCII text. It's essential when you need to transmit binary data through text-only channels or embed binary content in text formats.</p>
<p>Watch this quick explanation of how Base64 works (3 min):</p>
<p><a href="https://youtu.be/8qkxeZmKmOY" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/8qkxeZmKmOY/sddefault.webp" alt="What is Base64? (3 min)" class="img-fluid"></a></p>
<p>Here's how it works:</p>
<ul>
<li>It takes 3 bytes (24 bits) and converts them into 4 ASCII characters</li>
<li>... using 64 characters: A-Z, a-z, 0-9, + and / (padding with <code>=</code> to make the length a multiple of 4)</li>
<li>There's a URL-safe variant of Base64 that replaces + and / with - and _ to avoid issues in URLs</li>
<li>Base64 adds ~33% overhead (since every 3 bytes becomes 4 characters)</li>
</ul>
<p>Common Python operations with Base64:</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-keyword">import</span> base64

<span class="hljs-comment"># Basic encoding/decoding</span>
text = <span class="hljs-string">"Hello, World!"</span>
<span class="hljs-comment"># Convert text to base64</span>
encoded = base64.b64encode(text.encode()).decode()  <span class="hljs-comment"># SGVsbG8sIFdvcmxkIQ==</span>
<span class="hljs-comment"># Convert base64 back to text</span>
decoded = base64.b64decode(encoded).decode()        <span class="hljs-comment"># Hello, World!</span>
<span class="hljs-comment"># Convert to URL-safe base64</span>
url_safe = base64.urlsafe_b64encode(text.encode()).decode()  <span class="hljs-comment"># SGVsbG8sIFdvcmxkIQ==</span>

<span class="hljs-comment"># Working with binary files (e.g., images)</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'image.png'</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
    binary_data = f.read()
    image_b64 = base64.b64encode(binary_data).decode()

<span class="hljs-comment"># Data URI example (embed images in HTML/CSS)</span>
data_uri = <span class="hljs-string">f"data:image/png;base64,<span class="hljs-subst">{image_b64}</span>"</span>
</code></pre>
<p>Data URIs allow embedding binary data directly in HTML/CSS. This reduces the number of HTTP requests and also works offline. But it increases the file size.</p>
<p>For example, here's an SVG image embedded as a data URI:</p>
<pre><code class="language-html hljs language-xml" data-highlighted="yes"><span class="hljs-tag">&lt;<span class="hljs-name">img</span>
  <span class="hljs-attr">src</span>=<span class="hljs-string">"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAzMiAzMiI+PGNpcmNsZSBjeD0iMTYiIGN5PSIxNiIgcj0iMTUiIGZpbGw9IiMyNTYzZWIiLz48cGF0aCBmaWxsPSIjZmZmIiBkPSJtMTYgNyAyIDcgNyAyLTcgMi0yIDctMi03LTctMiA3LTJaIi8+PC9zdmc+"</span>
/&gt;</span>
</code></pre>
<p>Base64 is used in many places:</p>
<ul>
<li>JSON: Encoding binary data in JSON payloads</li>
<li>Email: MIME attachments encoding</li>
<li>Auth: HTTP Basic Authentication headers</li>
<li>JWT: Encoding tokens in web authentication</li>
<li>SSL/TLS: PEM certificate format</li>
<li>SAML: Encoding assertions in SSO</li>
<li>Git: Encoding binary files in patches</li>
</ul>
<p>Tools for working with Base64:</p>
<ul>
<li><a href="https://www.base64decode.org/" target="_blank" rel="noopener noreferrer">Base64 Decoder/Encoder</a> for online encoding/decoding</li>
<li><a href="https://dopiaza.org/tools/datauri/index.php" target="_blank" rel="noopener noreferrer">data: URI Generator</a> converts files to Data URIs</li>
</ul>
</div><!----><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>Vision Models</h2>
<p><a href="https://youtu.be/FgT_Mk_bakQ" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/FgT_Mk_bakQ/sddefault.webp" alt="LLM Vision Models" class="img-fluid"></a></p>
<p>You'll learn how to use LLMs to interpret images and extract useful information, covering:</p>
<ul>
<li><strong>Setting Up Vision Models</strong>: Integrate vision capabilities with LLMs using APIs like OpenAI's Chat Completion.</li>
<li><strong>Sending Image URLs for Analysis</strong>: Pass URLs or base64-encoded images to LLMs for processing.</li>
<li><strong>Reading Image Responses</strong>: Get detailed textual descriptions of images, from scenic landscapes to specific objects like cricketers or bank statements.</li>
<li><strong>Extracting Data from Images</strong>: Convert extracted image data to various formats like Markdown tables or JSON arrays.</li>
<li><strong>Handling Model Hallucinations</strong>: Address inaccuracies in extraction results, understanding how different prompts can affect output quality.</li>
<li><strong>Cost Management for Vision Models</strong>: Adjust detail settings (e.g., "detail: low") to balance cost and output precision.</li>
</ul>
<p>Here are the links used in the video:</p>
<ul>
<li><a href="https://colab.research.google.com/drive/1bK0b1XMrZWImtw01T1w9NGraDkiVi8mS" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a></li>
<li><a href="https://platform.openai.com/docs/api-reference/chat/create" target="_blank" rel="noopener noreferrer">OpenAI Chat API Reference</a></li>
<li><a href="https://platform.openai.com/docs/guides/vision" target="_blank" rel="noopener noreferrer">OpenAI Vision Guide</a></li>
<li><a href="https://drive.google.com/drive/folders/14MFc7XmGIUDU4-vbmF9305c1SSQrM-gR" target="_blank" rel="noopener noreferrer">Sample images used</a></li>
</ul>
<p>Here is an example of how to analyze an image using the OpenAI API.</p>
<pre><code class="language-bash hljs" data-highlighted="yes">curl https://api.openai.com/v1/chat/completions \
  -H <span class="hljs-string">"Content-Type: application/json"</span> \
  -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$OPENAI_API_KEY</span>"</span> \
  -d <span class="hljs-string">'{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "What is in this image?"},
          {
            "type": "image_url",
            "detail": "low",
            "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png"}
          }
        ]
      }
    ]
  }'</span>
</code></pre>
<p>Let's break down the request:</p>
<ul>
<li><code>curl https://api.openai.com/v1/chat/completions</code>: The API endpoint for text generation.</li>
<li><code>-H "Content-Type: application/json"</code>: The content type of the request.</li>
<li><code>-H "Authorization: Bearer $OPENAI_API_KEY"</code>: The API key for authentication.</li>
<li><code>-d</code>: The request body.<ul>
<li><code>"model": "gpt-4o-mini"</code>: The model to use for text generation.</li>
<li><code>"messages":</code>: The messages to send to the model.<ul>
<li><code>"role": "user"</code>: The role of the message.</li>
<li><code>"content":</code>: The content of the message.<ul>
<li><code>{"type": "text", "text": "What is in this image?"}</code>: The text message.</li>
<li><code>{"type": "image_url"}</code>: The image message.<ul>
<li><code>"detail": "low"</code>: The detail level of the image. <code>low</code> uses fewer tokens at lower detail. <code>high</code> uses more tokens for higher detail.</li>
<li><code>"image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png"}</code>: The URL of the image.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>You can send images in a <a href="base64-image.md" target="_blank" rel="noopener noreferrer">base64 encoded format</a>, too. For example:</p>
<pre><code class="language-bash hljs" data-highlighted="yes"><span class="hljs-comment"># Download image and convert to base64 in one step</span>
IMAGE_BASE64=$(curl -s <span class="hljs-string">"https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png"</span> | <span class="hljs-built_in">base64</span> -w 0)

<span class="hljs-comment"># Send to OpenAI API</span>
curl https://api.openai.com/v1/chat/completions \
  -H <span class="hljs-string">"Content-Type: application/json"</span> \
  -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$OPENAI_API_KEY</span>"</span> \
  -d @- &lt;&lt; <span class="hljs-string">EOF
{
  "model": "gpt-4o-mini",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What is in this image?"},
        {
          "type": "image_url",
          "image_url": { "url": "data:image/png;base64,$IMAGE_BASE64" }
        }
      ]
    }
  ]
}
EOF</span>
</code></pre>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <p>
      Acme Global Solutions manages hundreds of invoices from vendors every month. To streamline their accounts payable
      process, the company is developing an automated document processing system. This system uses a computer vision
      model to extract useful text from scanned invoice images. Critical pieces of data such as vendor email addresses,
      invoice or transaction numbers, and other details are embedded within these documents.
    </p>
    <p>
      Your team is tasked with integrating OpenAI's vision model into the invoice processing workflow. The chosen model,
      <code>gpt-4o-mini</code>, is capable of analyzing both text and image inputs simultaneously. When an invoice is
      received—for example, an invoice image may contain a vendor email like alice.brown@acmeglobal.com and a
      transaction number such as 34921. The system needs to extract all embedded text to automatically populate the
      vendor management system.
    </p>
    <p>The automated process will send a POST request to OpenAI's API with two inputs in a single user message:</p>
    <ol>
      <li>Text: A simple instruction "Extract text from this image."</li>
      <li>
        Image URL: A base64 URL representing the invoice image that might include the email and the transaction number
        among other details.
      </li>
    </ol>
    <p>Here is an example invoice image:</p>
    <p><img src="data:image/png;base64,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"></p>
    <p>
      Write <em>just the JSON body</em> (not the URL, nor headers) for the POST request that sends these two pieces of
      content (text and image URL) to the OpenAI API endpoint.
    </p>
    <ol>
      <li>Use <code>gpt-4o-mini</code> as the model.</li>
      <li>Send a single user message to the model that has a text and an image_url content (in that order).</li>
      <li>The text content should be <code>Extract text from this image</code>.</li>
      <li>Send the image_url as a base64 URL of the image above. <strong>CAREFUL</strong>: Do not modify the image.</li>
    </ol>

    <div class="mb-3">
      <label class="form-label" for="q-llm-vision">Write your JSON body here:</label>
      <textarea class="form-control font-monospace text-bg-dark" rows="6" id="q-llm-vision" name="q-llm-vision" disabled=""></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-llm-vision" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-llm-embeddings" id="hq-llm-embeddings">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->5</span>
              <!--?lit$697713494$-->LLM Embeddings (<!--?lit$697713494$-->0.75 <!--?lit$697713494$-->marks)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>Embeddings: OpenAI and Local Models</h2>
<p>Embedding models convert text into a list of numbers. These are like a map of text in numerical form. Each number represents a feature, and similar texts will have numbers close to each other. So, if the numbers are similar, the text they represent mean something similar.</p>
<p>This is useful because text similarity is important in many common problems:</p>
<ol>
<li><strong>Search</strong>. Find similar documents to a query.</li>
<li><strong>Classification</strong>. Classify text into categories.</li>
<li><strong>Clustering</strong>. Group similar items into clusters.</li>
<li><strong>Anomaly Detection</strong>. Find an unusual piece of text.</li>
</ol>
<p>You can run embedding models locally or using an API. Local models are better for privacy and cost. APIs are better for scale and quality.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Local Models</th>
<th>API</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Privacy</strong></td>
<td>High</td>
<td>Dependent on provider</td>
</tr>
<tr>
<td><strong>Cost</strong></td>
<td>High setup, low after that</td>
<td>Pay-as-you-go</td>
</tr>
<tr>
<td><strong>Scale</strong></td>
<td>Limited by local resources</td>
<td>Easily scales with demand</td>
</tr>
<tr>
<td><strong>Quality</strong></td>
<td>Varies by model</td>
<td>Typically high</td>
</tr>
</tbody></table>
<p>The <a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank" rel="noopener noreferrer">Massive Text Embedding Benchmark (MTEB)</a> provides comprehensive comparisons of embedding models. These models are compared on several parameters, but here are some key ones to look at:</p>
<ol>
<li><strong>Rank</strong>. Higher ranked models have higher quality.</li>
<li><strong>Memory Usage</strong>. Lower is better (for similar ranks). It costs less and is faster to run.</li>
<li><strong>Embedding Dimensions</strong>. Lower is better. This is the number of numbers in the array. Smaller dimensions are cheaper to store.</li>
<li><strong>Max Tokens</strong>. Higher is better. This is the number of input tokens (words) the model can take in a <em>single</em> input.</li>
<li>Look for higher scores in the columns for Classification, Clustering, Summarization, etc. based on your needs.</li>
</ol>
<h3>Local Embeddings</h3>
<p><a href="https://youtu.be/OATCgQtNX2o" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi/OATCgQtNX2o/sddefault.jpg" alt="Guide to Local Embeddings with Sentence Transformers" class="img-fluid"></a></p>
<p>Here's a minimal example using a local embedding model:</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-comment"># /// script</span>
<span class="hljs-comment"># requires-python = "==3.12"</span>
<span class="hljs-comment"># dependencies = [</span>
<span class="hljs-comment">#   "sentence-transformers",</span>
<span class="hljs-comment">#   "httpx",</span>
<span class="hljs-comment">#   "numpy",</span>
<span class="hljs-comment"># ]</span>
<span class="hljs-comment"># ///</span>

<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

model = SentenceTransformer(<span class="hljs-string">'BAAI/bge-base-en-v1.5'</span>)  <span class="hljs-comment"># A small, high quality model</span>

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">embed</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>]:
    <span class="hljs-string">"""Get embedding vector for text using local model."""</span>
    <span class="hljs-keyword">return</span> model.encode(text).tolist()

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_similarity</span>(<span class="hljs-params">text1: <span class="hljs-built_in">str</span>, text2: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">float</span>:
    <span class="hljs-string">"""Calculate cosine similarity between two texts."""</span>
    emb1 = np.array(<span class="hljs-keyword">await</span> embed(text1))
    emb2 = np.array(<span class="hljs-keyword">await</span> embed(text2))
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    <span class="hljs-built_in">print</span>(<span class="hljs-keyword">await</span> get_similarity(<span class="hljs-string">"Apple"</span>, <span class="hljs-string">"Orange"</span>))
    <span class="hljs-built_in">print</span>(<span class="hljs-keyword">await</span> get_similarity(<span class="hljs-string">"Apple"</span>, <span class="hljs-string">"Lightning"</span>))


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">import</span> asyncio
    asyncio.run(main())
</code></pre>
<p>Note the <code>get_similarity</code> function. It uses a <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener noreferrer">Cosine Similarity</a> to calculate the similarity between two embeddings.</p>
<h3>OpenAI Embeddings</h3>
<p>For comparison, here's how to use OpenAI's API with direct HTTP calls. Replace the <code>embed</code> function in the earlier script:</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> httpx

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">embed</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>]:
    <span class="hljs-string">"""Get embedding vector for text using OpenAI's API."""</span>
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> httpx.AsyncClient() <span class="hljs-keyword">as</span> client:
        response = <span class="hljs-keyword">await</span> client.post(
            <span class="hljs-string">"https://api.openai.com/v1/embeddings"</span>,
            headers={<span class="hljs-string">"Authorization"</span>: <span class="hljs-string">f"Bearer <span class="hljs-subst">{os.environ[<span class="hljs-string">'OPENAI_API_KEY'</span>]}</span>"</span>},
            json={<span class="hljs-string">"model"</span>: <span class="hljs-string">"text-embedding-3-small"</span>, <span class="hljs-string">"input"</span>: text}
        )
        <span class="hljs-keyword">return</span> response.json()[<span class="hljs-string">"data"</span>][<span class="hljs-number">0</span>][<span class="hljs-string">"embedding"</span>]
</code></pre>
<p><strong>NOTE</strong>: You need to set the <code>OPENAI_API_KEY</code> environment variable for this to work.</p>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <p>
      <strong>SecurePay</strong>, a leading fintech startup, has implemented an innovative feature to detect and prevent
      fraudulent activities in real time. As part of its security suite, the system analyzes personalized transaction
      messages by converting them into embeddings. These embeddings are compared against known patterns of legitimate
      and fraudulent messages to flag unusual activity.
    </p>
    <p>
      Imagine you are working on the SecurePay team as a junior developer tasked with integrating the text embeddings
      feature into the fraud detection module. When a user initiates a transaction, the system sends a personalized
      verification message to the user's registered email address. This message includes the user's email address and a
      unique transaction code (a randomly generated number). Here are 2 verification messages:
    </p>
    <pre><code data-highlighted="yes" class="hljs language-stylus">Dear user, please verify your transaction <span class="hljs-selector-tag">code</span> <span class="hljs-number">59147</span> sent to <span class="hljs-number">23</span>ds3000149@ds<span class="hljs-selector-class">.study</span><span class="hljs-selector-class">.iitm</span><span class="hljs-selector-class">.ac</span>.<span class="hljs-keyword">in</span></code></pre>
    <pre><code data-highlighted="yes" class="hljs language-stylus">Dear user, please verify your transaction <span class="hljs-selector-tag">code</span> <span class="hljs-number">96979</span> sent to <span class="hljs-number">23</span>ds3000149@ds<span class="hljs-selector-class">.study</span><span class="hljs-selector-class">.iitm</span><span class="hljs-selector-class">.ac</span>.<span class="hljs-keyword">in</span></code></pre>
    <p>
      The goal is to capture this message, convert it into a meaningful embedding using OpenAI's
      <code>text-embedding-3-small</code> model, and subsequently use the embedding in a machine learning model to
      detect anomalies.
    </p>
    <p>
      Your task is to write the JSON body for a POST request that will be sent to the OpenAI API endpoint to obtain the
      text embedding for the 2 given personalized transaction verification messages above. This will be sent to the
      endpoint
      <code>https://api.openai.com/v1/embeddings</code>.
    </p>

    <div class="mb-3">
      <label class="form-label" for="q-llm-embeddings">Write your JSON body here:</label>
      <textarea class="form-control font-monospace text-bg-dark" rows="6" id="q-llm-embeddings" name="q-llm-embeddings" disabled=""></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-llm-embeddings" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-embedding-similarity" id="hq-embedding-similarity">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->6</span>
              <!--?lit$697713494$-->Embedding Similarity (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>Topic Modeling</h2>
<p><a href="https://youtu.be/eQUNhq91DlI" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/eQUNhq91DlI/sddefault.webp" alt="LLM Topic Modeling" class="img-fluid"></a></p>
<p>You'll learn to use text embeddings to find text similarity and use that to create topics automatically from text, covering:</p>
<ul>
<li><strong>Embeddings</strong>: How large language models convert text into numerical representations.</li>
<li><strong>Similarity Measurement</strong>: Understanding how similar embeddings indicate similar meanings.</li>
<li><strong>Embedding Visualization</strong>: Using tools like Tensorflow Projector to visualize embedding spaces.</li>
<li><strong>Embedding Applications</strong>: Using embeddings for tasks like classification and clustering.</li>
<li><strong>OpenAI Embeddings</strong>: Using OpenAI's API to generate embeddings for text.</li>
<li><strong>Model Comparison</strong>: Exploring different embedding models and their strengths and weaknesses.</li>
<li><strong>Cosine Similarity</strong>: Calculating cosine similarity between embeddings for more reliable similarity measures.</li>
<li><strong>Embedding Cost</strong>: Understanding the cost of generating embeddings using OpenAI's API.</li>
<li><strong>Embedding Range</strong>: Understanding the range of values in embeddings and their significance.</li>
</ul>
<p>Here are the links used in the video:</p>
<ul>
<li><a href="https://colab.research.google.com/drive/15L075RLrwXkxa29EGT-1sNm_dqJRBTe_" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a></li>
<li><a href="https://projector.tensorflow.org/" target="_blank" rel="noopener noreferrer">Tensorflow projector</a></li>
<li><a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noopener noreferrer">Embeddings guide</a></li>
<li><a href="https://platform.openai.com/docs/api-reference/embeddings" target="_blank" rel="noopener noreferrer">Embeddings reference</a></li>
<li><a href="https://scikit-learn.org/stable/modules/clustering.html" target="_blank" rel="noopener noreferrer">Clustering on scikit-learn</a></li>
<li><a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank" rel="noopener noreferrer">Massive text embedding leaderboard (MTEB)</a></li>
<li><a href="https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5" target="_blank" rel="noopener noreferrer"><code>gte-large-en-v1.5</code> embedding model</a></li>
<li><a href="https://www.s-anand.net/blog/embeddings-similarity-threshold/" target="_blank" rel="noopener noreferrer">Embeddings similarity threshold</a></li>
</ul>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <p>
      ShopSmart is an online retail platform that places a high value on customer feedback. Each month, the company
      receives hundreds of comments from shoppers regarding product quality, delivery speed, customer service, and more.
      To automatically understand and cluster this feedback, ShopSmart's data science team uses text embeddings to
      capture the semantic meaning behind each comment.
    </p>
    <p>
      As part of a pilot project, ShopSmart has curated a collection of 25 feedback phrases that represent a variety of
      customer sentiments. Examples of these phrases include comments like “Fast shipping and great service,” “Product
      quality could be improved,” “Excellent packaging,” and so on. Due to limited processing capacity during initial
      testing, you have been tasked with determine which pair(s) of 5 of these phrases are most similar to each other.
      This similarity analysis will help in grouping similar feedback to enhance the company’s understanding of
      recurring customer issues.
    </p>
    <p>
      ShopSmart has written a Python program that has the 5 phrases and their embeddings as an array of floats. It looks
      like this:
    </p>
    <pre><code class="language-python hljs" data-highlighted="yes">embeddings = {<span class="hljs-string">"Product quality could be improved."</span>:[<span class="hljs-number">0.02994030900299549</span>,<span class="hljs-number">0.0700574517250061</span>,-<span class="hljs-number">0.09608972817659378</span>,<span class="hljs-number">0.0757998675107956</span>,<span class="hljs-number">0.05681799724698067</span>,-<span class="hljs-number">0.12199439853429794</span>,<span class="hljs-number">0.1026616021990776</span>,<span class="hljs-number">0.34097179770469666</span>,<span class="hljs-number">0.10221496969461441</span>,-<span class="hljs-number">0.022985607385635376</span>,<span class="hljs-number">0.00909215584397316</span>,-<span class="hljs-number">0.12154776602983475</span>,-<span class="hljs-number">0.33331525325775146</span>,-<span class="hljs-number">0.03502872586250305</span>,<span class="hljs-number">0.09934376925230026</span>,-<span class="hljs-number">0.07471518963575363</span>,<span class="hljs-number">0.232376366853714</span>,-<span class="hljs-number">0.1896272748708725</span>,-<span class="hljs-number">0.17048589885234833</span>,<span class="hljs-number">0.0928356945514679</span>,<span class="hljs-number">0.21285215020179749</span>,<span class="hljs-number">0.060550566762685776</span>,<span class="hljs-number">0.17584548890590668</span>,<span class="hljs-number">0.05365967005491257</span>,<span class="hljs-number">0.0439932718873024</span>,<span class="hljs-number">0.0900282934308052</span>,<span class="hljs-number">0.18656465411186218</span>,-<span class="hljs-number">0.18146029114723206</span>,-<span class="hljs-number">0.006986604072153568</span>,-<span class="hljs-number">0.11421024054288864</span>,<span class="hljs-number">0.14624014496803284</span>,-<span class="hljs-number">0.19919796288013458</span>,<span class="hljs-number">0.14802667498588562</span>,-<span class="hljs-number">0.062432803213596344</span>,-<span class="hljs-number">0.26695844531059265</span>,<span class="hljs-number">0.0347416065633297</span>,<span class="hljs-number">0.3560296893119812</span>,<span class="hljs-number">0.1255674511194229</span>,<span class="hljs-number">0.022554926574230194</span>,-<span class="hljs-number">0.060359153896570206</span>,-<span class="hljs-number">0.0147787407040596</span>,<span class="hljs-number">0.09608972817659378</span>,<span class="hljs-number">0.043897565454244614</span>,<span class="hljs-number">0.11484828591346741</span>,<span class="hljs-number">0.15619367361068726</span>,-<span class="hljs-number">0.04826818034052849</span>,<span class="hljs-number">0.020592935383319855</span>,-<span class="hljs-number">0.09813147783279419</span>,<span class="hljs-number">0.06405982375144958</span>,-<span class="hljs-number">0.08907122164964676</span>],<span class="hljs-string">"The product did not meet my expectations."</span>:[-<span class="hljs-number">0.0789492279291153</span>,-<span class="hljs-number">0.017544273287057877</span>,-<span class="hljs-number">0.20415154099464417</span>,<span class="hljs-number">0.05229542776942253</span>,-<span class="hljs-number">0.33714449405670166</span>,-<span class="hljs-number">0.0982111245393753</span>,<span class="hljs-number">0.12587708234786987</span>,<span class="hljs-number">0.11225880682468414</span>,-<span class="hljs-number">0.0027278736233711243</span>,<span class="hljs-number">0.023417923599481583</span>,-<span class="hljs-number">0.13826850056648254</span>,-<span class="hljs-number">0.291504830121994</span>,-<span class="hljs-number">0.18145440518856049</span>,-<span class="hljs-number">0.02094884216785431</span>,<span class="hljs-number">0.16108831763267517</span>,<span class="hljs-number">0.11158403009176254</span>,-<span class="hljs-number">0.012337733991444111</span>,-<span class="hljs-number">0.12710395455360413</span>,-<span class="hljs-number">0.3081902861595154</span>,<span class="hljs-number">0.03130057826638222</span>,<span class="hljs-number">0.03367764130234718</span>,<span class="hljs-number">0.21053127944469452</span>,-<span class="hljs-number">0.07563666999340057</span>,-<span class="hljs-number">0.1394953727722168</span>,-<span class="hljs-number">0.22488567233085632</span>,<span class="hljs-number">0.02143959142267704</span>,<span class="hljs-number">0.15299096703529358</span>,-<span class="hljs-number">0.07631145417690277</span>,<span class="hljs-number">0.011356235481798649</span>,<span class="hljs-number">0.15188677608966827</span>,<span class="hljs-number">0.042173732072114944</span>,-<span class="hljs-number">0.1614563763141632</span>,<span class="hljs-number">0.12152169644832611</span>,-<span class="hljs-number">0.29862070083618164</span>,<span class="hljs-number">0.09680021554231644</span>,<span class="hljs-number">0.09864052385091782</span>,<span class="hljs-number">0.11354702711105347</span>,-<span class="hljs-number">0.026837829500436783</span>,<span class="hljs-number">0.0004603167180903256</span>,<span class="hljs-number">0.1484515368938446</span>,<span class="hljs-number">0.014362072572112083</span>,<span class="hljs-number">0.04327791556715965</span>,-<span class="hljs-number">0.09661618620157242</span>,<span class="hljs-number">0.026745814830064774</span>,-<span class="hljs-number">0.12047885358333588</span>,<span class="hljs-number">0.252981036901474</span>,<span class="hljs-number">0.135446697473526</span>,-<span class="hljs-number">0.1340971291065216</span>,<span class="hljs-number">0.08907092362642288</span>,-<span class="hljs-number">0.11428314447402954</span>],<span class="hljs-string">"Packaging was excellent."</span>:[-<span class="hljs-number">0.01674579456448555</span>,-<span class="hljs-number">0.06481242924928665</span>,-<span class="hljs-number">0.24050545692443848</span>,<span class="hljs-number">0.042519159615039825</span>,<span class="hljs-number">0.14857585728168488</span>,-<span class="hljs-number">0.11343036592006683</span>,<span class="hljs-number">0.1299005150794983</span>,<span class="hljs-number">0.17366009950637817</span>,-<span class="hljs-number">0.12356054037809372</span>,<span class="hljs-number">0.049548257142305374</span>,<span class="hljs-number">0.23058201372623444</span>,-<span class="hljs-number">0.015152188017964363</span>,-<span class="hljs-number">0.06047092750668526</span>,-<span class="hljs-number">0.08428027480840683</span>,<span class="hljs-number">0.140513077378273</span>,<span class="hljs-number">0.0330953411757946</span>,<span class="hljs-number">0.15987755358219147</span>,-<span class="hljs-number">0.13982394337654114</span>,-<span class="hljs-number">0.1899235099554062</span>,<span class="hljs-number">0.0849694088101387</span>,<span class="hljs-number">0.10901995003223419</span>,<span class="hljs-number">0.023171907290816307</span>,<span class="hljs-number">0.1423737108707428</span>,-<span class="hljs-number">0.010603947564959526</span>,-<span class="hljs-number">0.12362945079803467</span>,-<span class="hljs-number">0.02598010189831257</span>,<span class="hljs-number">0.04410415142774582</span>,-<span class="hljs-number">0.0650191679596901</span>,<span class="hljs-number">0.13754981756210327</span>,<span class="hljs-number">0.06319297850131989</span>,<span class="hljs-number">0.2340276539325714</span>,-<span class="hljs-number">0.1448545753955841</span>,<span class="hljs-number">0.5634305477142334</span>,<span class="hljs-number">0.003012778703123331</span>,-<span class="hljs-number">0.15422670543193817</span>,-<span class="hljs-number">0.10137064009904861</span>,<span class="hljs-number">0.10013020783662796</span>,<span class="hljs-number">0.05392421782016754</span>,<span class="hljs-number">0.10895103961229324</span>,-<span class="hljs-number">0.017710573971271515</span>,-<span class="hljs-number">0.0018617206951603293</span>,<span class="hljs-number">0.01796899549663067</span>,<span class="hljs-number">0.0550268217921257</span>,<span class="hljs-number">0.251669317483902</span>,-<span class="hljs-number">0.005680993665009737</span>,<span class="hljs-number">0.12080402672290802</span>,-<span class="hljs-number">0.08173050731420517</span>,<span class="hljs-number">0.1045406237244606</span>,<span class="hljs-number">0.040589600801467896</span>,<span class="hljs-number">0.1787596344947815</span>],<span class="hljs-string">"Ordering was simple and straightforward."</span>:[-<span class="hljs-number">0.27091485261917114</span>,-<span class="hljs-number">0.16322025656700134</span>,-<span class="hljs-number">0.34741997718811035</span>,-<span class="hljs-number">0.20755687355995178</span>,<span class="hljs-number">0.07965204864740372</span>,-<span class="hljs-number">0.03790290653705597</span>,-<span class="hljs-number">0.07272882014513016</span>,<span class="hljs-number">0.14391915500164032</span>,-<span class="hljs-number">0.13000276684761047</span>,-<span class="hljs-number">0.01828710362315178</span>,<span class="hljs-number">0.15734601020812988</span>,-<span class="hljs-number">0.166996568441391</span>,<span class="hljs-number">0.0798618420958519</span>,-<span class="hljs-number">0.019056349992752075</span>,<span class="hljs-number">0.08161012828350067</span>,-<span class="hljs-number">0.11307933181524277</span>,-<span class="hljs-number">0.03884698078036308</span>,<span class="hljs-number">0.06776367872953415</span>,-<span class="hljs-number">0.09279917925596237</span>,<span class="hljs-number">0.14685627818107605</span>,<span class="hljs-number">0.12503762543201447</span>,-<span class="hljs-number">0.059197064489126205</span>,<span class="hljs-number">0.19636781513690948</span>,-<span class="hljs-number">0.21664796769618988</span>,-<span class="hljs-number">0.2507745623588562</span>,-<span class="hljs-number">0.22420057654380798</span>,-<span class="hljs-number">0.04014071449637413</span>,-<span class="hljs-number">0.21217235922813416</span>,-<span class="hljs-number">0.1732904016971588</span>,-<span class="hljs-number">0.09454746544361115</span>,<span class="hljs-number">0.19105301797389984</span>,-<span class="hljs-number">0.1433596909046173</span>,<span class="hljs-number">0.17720657587051392</span>,<span class="hljs-number">0.08419759571552277</span>,-<span class="hljs-number">0.10762467235326767</span>,<span class="hljs-number">0.06349785625934601</span>,<span class="hljs-number">0.07461697608232498</span>,<span class="hljs-number">0.1469961404800415</span>,<span class="hljs-number">0.15328997373580933</span>,<span class="hljs-number">0.05730891227722168</span>,-<span class="hljs-number">0.02755303494632244</span>,<span class="hljs-number">0.11391851305961609</span>,<span class="hljs-number">0.017002111300826073</span>,-<span class="hljs-number">0.05181928724050522</span>,-<span class="hljs-number">0.046399589627981186</span>,<span class="hljs-number">0.17776602506637573</span>,-<span class="hljs-number">0.19888535141944885</span>,<span class="hljs-number">0.10496727377176285</span>,-<span class="hljs-number">0.01117156632244587</span>,<span class="hljs-number">0.019633285701274872</span>],<span class="hljs-string">"The product description matched the item."</span>:[-<span class="hljs-number">0.1778346747159958</span>,<span class="hljs-number">0.015024187043309212</span>,-<span class="hljs-number">0.48206639289855957</span>,-<span class="hljs-number">0.025718823075294495</span>,-<span class="hljs-number">0.016542760655283928</span>,-<span class="hljs-number">0.14746320247650146</span>,<span class="hljs-number">0.08109830319881439</span>,<span class="hljs-number">0.14048422873020172</span>,-<span class="hljs-number">0.06655876338481903</span>,-<span class="hljs-number">0.014773784205317497</span>,-<span class="hljs-number">0.022116249427199364</span>,-<span class="hljs-number">0.09764105826616287</span>,<span class="hljs-number">0.0843939259648323</span>,-<span class="hljs-number">0.21104943752288818</span>,<span class="hljs-number">0.05166381597518921</span>,<span class="hljs-number">0.24917533993721008</span>,-<span class="hljs-number">0.04652651399374008</span>,-<span class="hljs-number">0.03644577041268349</span>,-<span class="hljs-number">0.3680764436721802</span>,<span class="hljs-number">0.14306902885437012</span>,<span class="hljs-number">0.19114643335342407</span>,<span class="hljs-number">0.09570245444774628</span>,<span class="hljs-number">0.12562158703804016</span>,<span class="hljs-number">0.04345705732703209</span>,-<span class="hljs-number">0.05486251413822174</span>,-<span class="hljs-number">0.1628427952528</span>,-<span class="hljs-number">0.04840049892663956</span>,-<span class="hljs-number">0.08885271847248077</span>,<span class="hljs-number">0.20407046377658844</span>,<span class="hljs-number">0.14849711954593658</span>,<span class="hljs-number">0.017899783328175545</span>,-<span class="hljs-number">0.17020949721336365</span>,<span class="hljs-number">0.13428069651126862</span>,-<span class="hljs-number">0.2234565168619156</span>,<span class="hljs-number">0.00254037999548018</span>,<span class="hljs-number">0.044975630939006805</span>,<span class="hljs-number">0.14862637221813202</span>,-<span class="hljs-number">0.06594487279653549</span>,<span class="hljs-number">0.15728546679019928</span>,<span class="hljs-number">0.006142953876405954</span>,-<span class="hljs-number">0.207172229886055</span>,-<span class="hljs-number">0.020533055067062378</span>,-<span class="hljs-number">0.05463634431362152</span>,<span class="hljs-number">0.09492701292037964</span>,-<span class="hljs-number">0.03237469866871834</span>,<span class="hljs-number">0.06752806901931763</span>,-<span class="hljs-number">0.08736645430326462</span>,<span class="hljs-number">0.08297228813171387</span>,-<span class="hljs-number">0.036898110061883926</span>,-<span class="hljs-number">0.045621830970048904</span>]}</code></pre>
    <p>
      Your task is to write a Python function <code>most_similar(embeddings)</code> that will calculate the cosine
      similarity between each pair of these embeddings and return the pair that has the highest similarity. The result
      should be a tuple of the two phrases that are most similar.
    </p>

    <div class="mb-3">
      <label class="form-label" for="q-embedding-similarity">Write your Python code here:</label>
      <textarea class="form-control font-monospace text-bg-dark" rows="6" id="q-embedding-similarity" name="q-embedding-similarity" disabled="">import numpy


def most_similar(embeddings):
    # Your code here
    return (phrase1, phrase2)
</textarea><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-embedding-similarity" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-vector-databases" id="hq-vector-databases">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->7</span>
              <!--?lit$697713494$-->Vector Databases (<!--?lit$697713494$-->1.5 <!--?lit$697713494$-->marks)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>Vector Databases</h2>
<p>Vector databases are specialized databases that store and search vector embeddings efficiently.</p>
<p>Use vector databases when your embeddings exceed available memory or when you want it run fast at scale. (This is important. If your code runs fast and fits in memory, you <strong>DON'T</strong> need a vector database. You can can use <code>numpy</code> for these tasks.)</p>
<p>Vector databases are an evolving space.</p>
<p>The first generation of vector databases were written in C and typically used an algorithm called <a href="https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world" target="_blank" rel="noopener noreferrer">HNSW</a> (a way to approximately find the nearest neighbor). Some popular ones are:</p>
<ul>
<li><strong><a href="https://docs.trychroma.com/" target="_blank" rel="noopener noreferrer">Chroma</a></strong>: Combines a vector index with a SQLite database. Easy to install, most popular.</li>
<li><strong><a href="https://lancedb.github.io/lancedb/" target="_blank" rel="noopener noreferrer">LanceDB</a></strong>: Written in Rust. Faster, easy to install, growing popular.</li>
<li><strong><a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener noreferrer">FAISS</a></strong>: Meta's lightweight library</li>
<li><strong><a href="https://milvus.io/" target="_blank" rel="noopener noreferrer">Milvus</a></strong>: Distributed, cloud-native</li>
</ul>
<p>In addition, most relational databases now support vector search. For example:</p>
<ul>
<li><strong><a href="https://duckdb.org/" target="_blank" rel="noopener noreferrer">DuckDB</a></strong>: Supports vector search with <a href="https://duckdb.org/docs/extensions/vss.html" target="_blank" rel="noopener noreferrer"><code>vss</code></a>.</li>
<li><strong><a href="https://www.sqlite.org/" target="_blank" rel="noopener noreferrer">SQLite</a></strong>: Supports vector search with <a href="https://github.com/asg017/sqlite-vec" target="_blank" rel="noopener noreferrer"><code>sqlite-vec</code></a>.</li>
<li><strong><a href="https://www.postgresql.org/" target="_blank" rel="noopener noreferrer">PostgreSQL</a></strong>: Supports vector search with <a href="https://github.com/pgvector/pgvector" target="_blank" rel="noopener noreferrer"><code>pgvector</code></a>.</li>
</ul>
<p>Take a look at this <a href="https://superlinked.com/vector-db-comparison" target="_blank" rel="noopener noreferrer">Vector DB Comparison</a>.</p>
<p>Watch this Vector Database Tutorial (3 min):</p>
<p><a href="https://youtu.be/klTvEwg3oJ4" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi/klTvEwg3oJ4/sddefault.jpg" alt="Vector databases are so hot right now. WTF are they? (3 min)" class="img-fluid"></a></p>
<h3>ChromaDB</h3>
<p>Here's a minimal example using Chroma:</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-comment"># /// script</span>
<span class="hljs-comment"># requires-python = "==3.12"</span>
<span class="hljs-comment"># dependencies = [</span>
<span class="hljs-comment">#   "chromadb",</span>
<span class="hljs-comment">#   "sentence-transformers",</span>
<span class="hljs-comment"># ]</span>
<span class="hljs-comment"># ///</span>

<span class="hljs-keyword">import</span> chromadb
<span class="hljs-keyword">from</span> chromadb.utils <span class="hljs-keyword">import</span> embedding_functions
<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_vector_db</span>():
    <span class="hljs-string">"""Initialize Chroma DB with an embedding function."""</span>
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=<span class="hljs-string">"BAAI/bge-base-en-v1.5"</span>
    )
    client = chromadb.PersistentClient(path=<span class="hljs-string">"./vector_db"</span>)
    collection = client.create_collection(
        name=<span class="hljs-string">"documents"</span>,
        embedding_function=sentence_transformer_ef
    )
    <span class="hljs-keyword">return</span> collection

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">search_similar</span>(<span class="hljs-params">collection, query: <span class="hljs-built_in">str</span>, n_results: <span class="hljs-built_in">int</span> = <span class="hljs-number">3</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>]:
    <span class="hljs-string">"""Search for documents similar to the query."""</span>
    d = collection.query(query_texts=[query], n_results=n_results)
    <span class="hljs-keyword">return</span> [
        {<span class="hljs-string">"id"</span>: <span class="hljs-built_in">id</span>, <span class="hljs-string">"text"</span>: text, <span class="hljs-string">"distance"</span>: distance}
        <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span>, text, distance <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(d[<span class="hljs-string">"ids"</span>][<span class="hljs-number">0</span>], d[<span class="hljs-string">"documents"</span>][<span class="hljs-number">0</span>], d[<span class="hljs-string">"distances"</span>][<span class="hljs-number">0</span>])
    ]

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    collection = <span class="hljs-keyword">await</span> setup_vector_db()

    <span class="hljs-comment"># Add some documents</span>
    collection.add(
        documents=[<span class="hljs-string">"Apple is a fruit"</span>, <span class="hljs-string">"Orange is citrus"</span>, <span class="hljs-string">"Computer is electronic"</span>],
        ids=[<span class="hljs-string">"1"</span>, <span class="hljs-string">"2"</span>, <span class="hljs-string">"3"</span>]
    )

    <span class="hljs-comment"># Search</span>
    results = <span class="hljs-keyword">await</span> search_similar(collection, <span class="hljs-string">"fruit"</span>)
    <span class="hljs-built_in">print</span>(results)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">import</span> asyncio
    asyncio.run(main())
</code></pre>
<h3>LanceDB</h3>
<p>Here's the same example using LanceDB:</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-comment"># /// script</span>
<span class="hljs-comment"># requires-python = "==3.12"</span>
<span class="hljs-comment"># dependencies = [</span>
<span class="hljs-comment">#   "lancedb",</span>
<span class="hljs-comment">#   "pyarrow",</span>
<span class="hljs-comment">#   "sentence-transformers",</span>
<span class="hljs-comment"># ]</span>
<span class="hljs-comment"># ///</span>

<span class="hljs-keyword">import</span> lancedb
<span class="hljs-keyword">import</span> pyarrow <span class="hljs-keyword">as</span> pa
<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_vector_db</span>():
    <span class="hljs-string">"""Initialize LanceDB with an embedding function."""</span>
    model = SentenceTransformer(<span class="hljs-string">"BAAI/bge-base-en-v1.5"</span>)
    db = lancedb.connect(<span class="hljs-string">"./vector_db"</span>)

    <span class="hljs-comment"># Create table with schema for documents</span>
    table = db.create_table(
        <span class="hljs-string">"documents"</span>,
        schema=pa.schema([
            pa.field(<span class="hljs-string">"id"</span>, pa.string()),
            pa.field(<span class="hljs-string">"text"</span>, pa.string()),
            pa.field(<span class="hljs-string">"vector"</span>, pa.list_(pa.float32(), list_size=<span class="hljs-number">768</span>))
        ])
    )
    <span class="hljs-keyword">return</span> table, model

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">search_similar</span>(<span class="hljs-params">table, model, query: <span class="hljs-built_in">str</span>, n_results: <span class="hljs-built_in">int</span> = <span class="hljs-number">3</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>]:
    <span class="hljs-string">"""Search for documents similar to the query."""</span>
    query_embedding = model.encode(query)
    results = table.search(query_embedding).limit(n_results).to_list()
    <span class="hljs-keyword">return</span> [{<span class="hljs-string">"id"</span>: r[<span class="hljs-string">"id"</span>], <span class="hljs-string">"text"</span>: r[<span class="hljs-string">"text"</span>], <span class="hljs-string">"distance"</span>: <span class="hljs-built_in">float</span>(r[<span class="hljs-string">"_distance"</span>])} <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> results]

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    table, model = <span class="hljs-keyword">await</span> setup_vector_db()

    <span class="hljs-comment"># Add some documents</span>
    documents = [<span class="hljs-string">"Apple is a fruit"</span>, <span class="hljs-string">"Orange is citrus"</span>, <span class="hljs-string">"Computer is electronic"</span>]
    embeddings = model.encode(documents)

    table.add(data=[
        {<span class="hljs-string">"id"</span>: <span class="hljs-built_in">str</span>(i), <span class="hljs-string">"text"</span>: text, <span class="hljs-string">"vector"</span>: embedding}
        <span class="hljs-keyword">for</span> i, (text, embedding) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(documents, embeddings), <span class="hljs-number">1</span>)
    ])

    <span class="hljs-comment"># Search</span>
    results = <span class="hljs-keyword">await</span> search_similar(table, model, <span class="hljs-string">"fruit"</span>)
    <span class="hljs-built_in">print</span>(results)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">import</span> asyncio
    asyncio.run(main())
</code></pre>
<h3>DuckDB</h3>
<p>Here's the same example using DuckDB:</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-comment"># /// script</span>
<span class="hljs-comment"># requires-python = "==3.12"</span>
<span class="hljs-comment"># dependencies = [</span>
<span class="hljs-comment">#   "duckdb",</span>
<span class="hljs-comment">#   "sentence-transformers",</span>
<span class="hljs-comment"># ]</span>
<span class="hljs-comment"># ///</span>

<span class="hljs-keyword">import</span> duckdb
<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_vector_db</span>() -&gt; <span class="hljs-built_in">tuple</span>[duckdb.DuckDBPyConnection, SentenceTransformer]:
    <span class="hljs-string">"""Initialize DuckDB with VSS extension and embedding model."""</span>
    <span class="hljs-comment"># Initialize model</span>
    model = SentenceTransformer(<span class="hljs-string">"BAAI/bge-base-en-v1.5"</span>)
    vector_dim = model.get_sentence_embedding_dimension()

    <span class="hljs-comment"># Setup DuckDB with VSS extension</span>
    conn = duckdb.connect(<span class="hljs-string">":memory:"</span>)
    conn.install_extension(<span class="hljs-string">"vss"</span>)
    conn.load_extension(<span class="hljs-string">"vss"</span>)

    <span class="hljs-comment"># Create table with vector column</span>
    conn.execute(<span class="hljs-string">f"""
        CREATE TABLE documents (
            id VARCHAR,
            text VARCHAR,
            vector FLOAT[<span class="hljs-subst">{vector_dim}</span>]
        )
    """</span>)

    <span class="hljs-comment"># Create HNSW index for vector similarity search</span>
    conn.execute(<span class="hljs-string">"CREATE INDEX vector_idx ON documents USING HNSW (vector)"</span>)
    <span class="hljs-keyword">return</span> conn, model

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">search_similar</span>(<span class="hljs-params">conn: duckdb.DuckDBPyConnection, model: SentenceTransformer,
                        query: <span class="hljs-built_in">str</span>, n_results: <span class="hljs-built_in">int</span> = <span class="hljs-number">3</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>]:
    <span class="hljs-string">"""Search for documents similar to query using vector similarity."""</span>
    <span class="hljs-comment"># Encode query to vector</span>
    query_vector = model.encode(query).tolist()

    <span class="hljs-comment"># Search using HNSW index with explicit FLOAT[] cast</span>
    results = conn.execute(<span class="hljs-string">"""
        SELECT id, text, array_distance(vector, CAST(? AS FLOAT[768])) as distance
        FROM documents
        ORDER BY array_distance(vector, CAST(? AS FLOAT[768]))
        LIMIT ?
    """</span>, [query_vector, query_vector, n_results]).fetchall()

    <span class="hljs-keyword">return</span> [{<span class="hljs-string">"id"</span>: r[<span class="hljs-number">0</span>], <span class="hljs-string">"text"</span>: r[<span class="hljs-number">1</span>], <span class="hljs-string">"distance"</span>: <span class="hljs-built_in">float</span>(r[<span class="hljs-number">2</span>])} <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> results]

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    conn, model = <span class="hljs-keyword">await</span> setup_vector_db()

    <span class="hljs-comment"># Add sample documents</span>
    documents = [<span class="hljs-string">"Apple is a fruit"</span>, <span class="hljs-string">"Orange is citrus"</span>, <span class="hljs-string">"Computer is electronic"</span>]
    embeddings = model.encode(documents).tolist()

    <span class="hljs-comment"># Insert documents and vectors</span>
    conn.executemany(<span class="hljs-string">"""
        INSERT INTO documents (id, text, vector)
        VALUES (?, ?, ?)
    """</span>, [(<span class="hljs-built_in">str</span>(i), text, embedding)
          <span class="hljs-keyword">for</span> i, (text, embedding) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(documents, embeddings), <span class="hljs-number">1</span>)])

    <span class="hljs-comment"># Search similar documents</span>
    results = <span class="hljs-keyword">await</span> search_similar(conn, model, <span class="hljs-string">"fruit"</span>)
    <span class="hljs-built_in">print</span>(results)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">import</span> asyncio
    asyncio.run(main())
</code></pre>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <div class="mb-3">
      <p>
        InfoCore Solutions is a technology consulting firm that maintains an extensive internal knowledge base of
        technical documents, project reports, and case studies. Employees frequently search through these documents to
        answer client questions quickly or gain insights for ongoing projects. However, due to the sheer volume of
        documentation, traditional keyword-based search often returns too many irrelevant results.
      </p>
      <p>
        To address this issue, InfoCore's data science team decides to integrate a semantic search feature into their
        internal portal. This feature uses text embeddings to capture the contextual meaning of both the documents and
        the user's query. The documents are pre-embedded, and when an employee submits a search query, the system
        computes the similarity between the query's embedding and those of the documents. The API then returns a ranked
        list of document identifiers based on similarity.
      </p>
      <p>
        Imagine you are an engineer on the InfoCore team. Your task is to build a FastAPI POST endpoint that accepts an
        array of docs and query string via a JSON body. The endpoint is structured as follows:
      </p>
      <pre><code data-highlighted="yes" class="hljs language-jboss-cli">POST <span class="hljs-string">/similarity</span>

{
  <span class="hljs-string">"docs"</span>: [<span class="hljs-string">"Contents of document 1"</span>, <span class="hljs-string">"Contents of document 2"</span>, <span class="hljs-string">"Contents of document 3"</span>, <span class="hljs-string">...</span>],
  <span class="hljs-string">"query"</span>: <span class="hljs-string">"Your query string"</span>
}
</code></pre>

      <p><strong>Service Flow:</strong></p>
      <ol>
        <li>
          <strong>Request Payload:</strong> The client sends a POST request with a JSON body containing:
          <ul>
            <li>
              <strong><code>docs</code>:</strong> An array of document texts from the internal knowledge base.
            </li>
            <li>
              <strong><code>query</code>:</strong> A string representing the user's search query.
            </li>
          </ul>
        </li>
        <li>
          <strong>Embedding Generation:</strong> For each document in the <code>docs</code> array and for the
          <code>query</code> string, the API computes a text embedding using <code>text-embedding-3-small</code>.
        </li>
        <li>
          <strong>Similarity Computation:</strong> The API then calculates the cosine similarity between the query
          embedding and each document embedding. This allows the service to determine which documents best match the
          intent of the query.
        </li>
        <li>
          <p>
            <strong>Response Structure:</strong> After ranking the documents by their similarity scores, the API returns
            the identifiers (or positions) of the three most similar documents. The JSON response might look like this:
          </p>
          <pre><code class="language-json hljs" data-highlighted="yes"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">"matches"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">"Contents of document 3"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Contents of document 1"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Contents of document 2"</span><span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
          <p>
            Here, <code>"Contents of document 3"</code> is considered the closest match, followed by
            <code>"Contents of document 1"</code>, then <code>"Contents of document 2"</code>.
          </p>
        </li>
      </ol>
      <p>
        Make sure you enable CORS to allow <code>OPTIONS</code> and <code>POST</code> methods, perhaps allowing all
        origins and headers.
      </p>

      <div class="mb-3">
        <label class="form-label" for="q-vector-databases">
          What is the API URL endpoint for your implementation? It might look like:
          <code>http://127.0.0.1:8000/similarity</code>
        </label>
        <input class="form-control" type="url" required="" id="q-vector-databases" name="q-vector-databases" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        <p class="text-muted">
          We'll check by sending a POST request to this URL with a JSON body containing random <code>docs</code> and
          <code>query</code>.
        </p>
      </div>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-vector-databases" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-function-calling" id="hq-function-calling">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->8</span>
              <!--?lit$697713494$-->Function Calling (<!--?lit$697713494$-->1.5 <!--?lit$697713494$-->marks)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>Function Calling with OpenAI</h2>
<p><a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener noreferrer">Function Calling</a> allows Large Language Models to convert natural language into structured function calls. This is perfect for building chatbots and AI assistants that need to interact with your backend systems.</p>
<p>OpenAI supports <a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener noreferrer">Function Calling</a> -- a way for LLMs to suggest what functions to call and how.</p>
<p><a href="https://youtu.be/aqdWSYWC_LI" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/aqdWSYWC_LI/sddefault.webp" alt="OpenAI Function Calling - Full Beginner Tutorial" class="img-fluid"></a></p>
<p>Here's a minimal example using Python and OpenAI's function calling that identifies the weather in a given location.</p>
<pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-comment"># /// script</span>
<span class="hljs-comment"># requires-python = "&gt;=3.11"</span>
<span class="hljs-comment"># dependencies = [</span>
<span class="hljs-comment">#   "httpx",</span>
<span class="hljs-comment"># ]</span>
<span class="hljs-comment"># ///</span>

<span class="hljs-keyword">import</span> httpx
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, <span class="hljs-type">Any</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">query_gpt</span>(<span class="hljs-params">user_input: <span class="hljs-built_in">str</span>, tools: <span class="hljs-built_in">list</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
    response = httpx.post(
        <span class="hljs-string">"https://api.openai.com/v1/chat/completions"</span>,
        headers={
            <span class="hljs-string">"Authorization"</span>: <span class="hljs-string">f"Bearer <span class="hljs-subst">{os.getenv(<span class="hljs-string">'OPENAI_API_KEY'</span>)}</span>"</span>,
            <span class="hljs-string">"Content-Type"</span>: <span class="hljs-string">"application/json"</span>,
        },
        json={
            <span class="hljs-string">"model"</span>: <span class="hljs-string">"gpt-4o-mini"</span>,
            <span class="hljs-string">"messages"</span>: [{<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: user_input}],
            <span class="hljs-string">"tools"</span>: tools,
            <span class="hljs-string">"tool_choice"</span>: <span class="hljs-string">"auto"</span>,
        },
    )
    <span class="hljs-keyword">return</span> response.json()[<span class="hljs-string">"choices"</span>][<span class="hljs-number">0</span>][<span class="hljs-string">"message"</span>]


WEATHER_TOOL = {
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"function"</span>,
    <span class="hljs-string">"function"</span>: {
        <span class="hljs-string">"name"</span>: <span class="hljs-string">"get_weather"</span>,
        <span class="hljs-string">"description"</span>: <span class="hljs-string">"Get the current weather for a location"</span>,
        <span class="hljs-string">"parameters"</span>: {
            <span class="hljs-string">"type"</span>: <span class="hljs-string">"object"</span>,
            <span class="hljs-string">"properties"</span>: {
                <span class="hljs-string">"location"</span>: {<span class="hljs-string">"type"</span>: <span class="hljs-string">"string"</span>, <span class="hljs-string">"description"</span>: <span class="hljs-string">"City name or coordinates"</span>}
            },
            <span class="hljs-string">"required"</span>: [<span class="hljs-string">"location"</span>],
            <span class="hljs-string">"additionalProperties"</span>: <span class="hljs-literal">False</span>,
        },
        <span class="hljs-string">"strict"</span>: <span class="hljs-literal">True</span>,
    },
}

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    response = query_gpt(<span class="hljs-string">"What is the weather in San Francisco?"</span>, [WEATHER_TOOL])
    <span class="hljs-built_in">print</span>([tool_call[<span class="hljs-string">"function"</span>] <span class="hljs-keyword">for</span> tool_call <span class="hljs-keyword">in</span> response[<span class="hljs-string">"tool_calls"</span>]])
</code></pre>
<h3>How to define functions</h3>
<p>The function definition is a <a href="https://json-schema.org/" target="_blank" rel="noopener noreferrer">JSON schema</a> with a few OpenAI specific properties.
See the <a href="https://platform.openai.com/docs/guides/structured-outputs#supported-schemas" target="_blank" rel="noopener noreferrer">Supported schemas</a>.</p>
<p>Here's an example of a function definition for scheduling a meeting:</p>
<pre><code class="language-python hljs" data-highlighted="yes">MEETING_TOOL = {
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"function"</span>,
    <span class="hljs-string">"function"</span>: {
        <span class="hljs-string">"name"</span>: <span class="hljs-string">"schedule_meeting"</span>,
        <span class="hljs-string">"description"</span>: <span class="hljs-string">"Schedule a meeting room for a specific date and time"</span>,
        <span class="hljs-string">"parameters"</span>: {
            <span class="hljs-string">"type"</span>: <span class="hljs-string">"object"</span>,
            <span class="hljs-string">"properties"</span>: {
                <span class="hljs-string">"date"</span>: {
                    <span class="hljs-string">"type"</span>: <span class="hljs-string">"string"</span>,
                    <span class="hljs-string">"description"</span>: <span class="hljs-string">"Meeting date in YYYY-MM-DD format"</span>
                },
                <span class="hljs-string">"time"</span>: {
                    <span class="hljs-string">"type"</span>: <span class="hljs-string">"string"</span>,
                    <span class="hljs-string">"description"</span>: <span class="hljs-string">"Meeting time in HH:MM format"</span>
                },
                <span class="hljs-string">"meeting_room"</span>: {
                    <span class="hljs-string">"type"</span>: <span class="hljs-string">"string"</span>,
                    <span class="hljs-string">"description"</span>: <span class="hljs-string">"Name of the meeting room"</span>
                }
            },
            <span class="hljs-string">"required"</span>: [<span class="hljs-string">"date"</span>, <span class="hljs-string">"time"</span>, <span class="hljs-string">"meeting_room"</span>],
            <span class="hljs-string">"additionalProperties"</span>: <span class="hljs-literal">False</span>
        },
        <span class="hljs-string">"strict"</span>: <span class="hljs-literal">True</span>
    }
}
</code></pre>
<h3>How to define multiple functions</h3>
<p>You can define multiple functions by passing a list of function definitions to the <code>tools</code> parameter.</p>
<p>Here's an example of a list of function definitions for handling employee expenses and calculating performance bonuses:</p>
<pre><code class="language-python hljs" data-highlighted="yes">tools = [
    {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"function"</span>,
        <span class="hljs-string">"function"</span>: {
            <span class="hljs-string">"name"</span>: <span class="hljs-string">"get_expense_balance"</span>,
            <span class="hljs-string">"description"</span>: <span class="hljs-string">"Get expense balance for an employee"</span>,
            <span class="hljs-string">"parameters"</span>: {
                <span class="hljs-string">"type"</span>: <span class="hljs-string">"object"</span>,
                <span class="hljs-string">"properties"</span>: {
                    <span class="hljs-string">"employee_id"</span>: {
                        <span class="hljs-string">"type"</span>: <span class="hljs-string">"integer"</span>,
                        <span class="hljs-string">"description"</span>: <span class="hljs-string">"Employee ID number"</span>
                    }
                },
                <span class="hljs-string">"required"</span>: [<span class="hljs-string">"employee_id"</span>],
                <span class="hljs-string">"additionalProperties"</span>: <span class="hljs-literal">False</span>
            },
            <span class="hljs-string">"strict"</span>: <span class="hljs-literal">True</span>
        }
    },
    {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"function"</span>,
        <span class="hljs-string">"function"</span>: {
            <span class="hljs-string">"name"</span>: <span class="hljs-string">"calculate_performance_bonus"</span>,
            <span class="hljs-string">"description"</span>: <span class="hljs-string">"Calculate yearly performance bonus for an employee"</span>,
            <span class="hljs-string">"parameters"</span>: {
                <span class="hljs-string">"type"</span>: <span class="hljs-string">"object"</span>,
                <span class="hljs-string">"properties"</span>: {
                    <span class="hljs-string">"employee_id"</span>: {
                        <span class="hljs-string">"type"</span>: <span class="hljs-string">"integer"</span>,
                        <span class="hljs-string">"description"</span>: <span class="hljs-string">"Employee ID number"</span>
                    },
                    <span class="hljs-string">"current_year"</span>: {
                        <span class="hljs-string">"type"</span>: <span class="hljs-string">"integer"</span>,
                        <span class="hljs-string">"description"</span>: <span class="hljs-string">"Year to calculate bonus for"</span>
                    }
                },
                <span class="hljs-string">"required"</span>: [<span class="hljs-string">"employee_id"</span>, <span class="hljs-string">"current_year"</span>],
                <span class="hljs-string">"additionalProperties"</span>: <span class="hljs-literal">False</span>
            },
            <span class="hljs-string">"strict"</span>: <span class="hljs-literal">True</span>
        }
    }
]
</code></pre>
<p>Best Practices:</p>
<ol>
<li><strong>Use Strict Mode</strong><ul>
<li>Always set <code>strict: True</code> to ensure valid function calls</li>
<li>Define all required parameters</li>
<li>Set <code>additionalProperties: False</code></li>
</ul>
</li>
<li><strong>Use tool choice</strong><ul>
<li>Set <code>tool_choice: "required"</code> to ensure that the model will always call one or more tools</li>
<li>The default is <code>tool_choice: "auto"</code> which means the model will choose a tool only if appropriate</li>
</ul>
</li>
<li><strong>Clear Descriptions</strong><ul>
<li>Write detailed function and parameter descriptions</li>
<li>Include expected formats and units</li>
<li>Mention any constraints or limitations</li>
</ul>
</li>
<li><strong>Error Handling</strong><ul>
<li>Validate function inputs before execution</li>
<li>Return clear error messages</li>
<li>Handle missing or invalid parameters</li>
</ul>
</li>
</ol>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <div class="mb-3">
      <p>
        <strong>TechNova Corp.</strong> is a multinational corporation that has implemented a digital assistant to
        support employees with various internal tasks. The assistant can answer queries related to human resources, IT
        support, and administrative services. Employees use a simple web interface to enter their requests, which may
        include:
      </p>
      <ul>
        <li>Checking the status of an IT support ticket.</li>
        <li>Scheduling a meeting.</li>
        <li>Retrieving their current expense reimbursement balance.</li>
        <li>Requesting details about their performance bonus.</li>
        <li>Reporting an office issue by specifying a department or issue number.</li>
      </ul>
      <p>
        Each question is direct and templatized, containing one or more parameters such as an employee or ticket number
        (which might be randomized). In the backend, a FastAPI app routes each request by matching the query to one of a
        set of pre-defined functions. The response that the API returns is used by OpenAI to call the right function
        with the necessary arguments.
      </p>
      <p><strong>Pre-Defined Functions:</strong></p>
      <p>For this exercise, assume the following functions have been defined:</p>
      <ul>
        <li><code>get_ticket_status(ticket_id: int)</code></li>
        <li><code>schedule_meeting(date: str, time: str, meeting_room: str)</code></li>
        <li><code>get_expense_balance(employee_id: int)</code></li>
        <li><code>calculate_performance_bonus(employee_id: int, current_year: int)</code></li>
        <li><code>report_office_issue(issue_code: int, department: str)</code></li>
      </ul>
      <p>
        Each function has a specific signature, and the student’s FastAPI app should map specific queries to these
        functions.
      </p>
      <p><strong>Example Questions (Templatized with a Random Number):</strong></p>
      <ol>
        <li>
          <strong>Ticket Status:</strong><br>Query: <code>"What is the status of ticket 83742?"</code><br>→ Should
          map to <code>get_ticket_status(ticket_id=83742)</code>
        </li>
        <li>
          <strong>Meeting Scheduling:</strong><br>Query:
          <code>"Schedule a meeting on 2025-02-15 at 14:00 in Room A."</code><br>→ Should map to
          <code>schedule_meeting(date="2025-02-15", time="14:00", meeting_room="Room A")</code>
        </li>
        <li>
          <strong>Expense Reimbursement:</strong><br>Query: <code>"Show my expense balance for employee 10056."</code><br>→ Should map to
          <code>get_expense_balance(employee_id=10056)</code>
        </li>
        <li>
          <strong>Performance Bonus Calculation:</strong><br>Query:
          <code>"Calculate performance bonus for employee 10056 for 2025."</code><br>→ Should map to
          <code>calculate_performance_bonus(employee_id=10056, current_year=2025)</code>
        </li>
        <li>
          <strong>Office Issue Reporting:</strong><br>Query:
          <code>"Report office issue 45321 for the Facilities department."</code><br>→ Should map to
          <code>report_office_issue(issue_code=45321, department="Facilities")</code>
        </li>
      </ol>
      <p><strong>Task Overview:</strong></p>
      <p>Develop a FastAPI application that:</p>
      <ol>
        <li>
          Exposes a GET endpoint <code>/execute?q=...</code> where the query parameter <code>q</code> contains one of
          the pre-templatized questions.
        </li>
        <li>Analyzes the <code>q</code> parameter to identify which function should be called.</li>
        <li>Extracts the parameters from the question text.</li>
        <li>
          <p>Returns a response in the following JSON format:</p>
          <pre><code class="language-json hljs" data-highlighted="yes"><span class="hljs-punctuation">{</span> <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"function_name"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"arguments"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"{ ...JSON encoded parameters... }"</span> <span class="hljs-punctuation">}</span></code></pre>
        </li>
      </ol>
      <p>For example, the query <code>"What is the status of ticket 83742?"</code> should return:</p>
      <pre><code class="language-json hljs" data-highlighted="yes"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"get_ticket_status"</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">"arguments"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"{\"ticket_id\": 83742}"</span>
<span class="hljs-punctuation">}</span></code></pre>
      <p>Make sure you enable <strong>CORS</strong> to allow GET requests from any origin.</p>

      <div class="mb-3">
        <label class="form-label" for="q-function-calling">
          What is the API URL endpoint for your implementation? It might look like:
          <code>http://127.0.0.1:8000/execute</code>
        </label>
        <input class="form-control" type="url" required="" id="q-function-calling" name="q-function-calling" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        <p class="text-muted">
          We'll check by sending a GET request to this URL with <code>?q=...</code> containing a task. We'll verify that
          it matches the expected response. Arguments must be in the same order as the function definition.
        </p>
      </div>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-function-calling" disabled="">Check</button>
            </div>
          </div>
        <!----><!---->
          <div class="card my-5" data-question="q-get-llm-to-say-yes" id="hq-get-llm-to-say-yes">
            <div class="card-header">
              <span class="badge text-bg-primary me-2"><!--?lit$697713494$-->9</span>
              <!--?lit$697713494$-->Get an LLM to say Yes (<!--?lit$697713494$-->1 <!--?lit$697713494$-->mark)
            </div>
            <!--?lit$697713494$--><!----><div class="card-body border-bottom"><!--?lit$697713494$--><h2>Prompt Engineering</h2>
<p>Prompt engineering is the process of crafting effective prompts for large language models (LLMs).</p>
<p>One of the best ways to approach prompt engineering is to think of LLMs as a smart colleague (with amnesia) who needs explicit instructions.</p>
<p>The most authoritative guides are from the LLM providers themselves:</p>
<ul>
<li><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/" target="_blank" rel="noopener noreferrer">Anthropic</a></li>
<li><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design" target="_blank" rel="noopener noreferrer">Google</a></li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">OpenAI</a></li>
</ul>
<p>Here are some best practices:</p>
<h3>Use prompt optimizers</h3>
<p>They rewrite your prompt to improve it. Explore:</p>
<ul>
<li><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver" target="_blank" rel="noopener noreferrer">Anthropic Prompt Optimizer</a></li>
<li><a href="https://platform.openai.com/docs/guides/prompt-generation" target="_blank" rel="noopener noreferrer">OpenAI Prompt Generation</a></li>
<li><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/ai-powered-prompt-writing" target="_blank" rel="noopener noreferrer">Google AI-powered prompt writing tools</a></li>
</ul>
<h3>Be clear, direct, and detailed</h3>
<p>Be explicit and thorough. Include all necessary context, goals, and details so the model understands the full picture.</p>
<ul>
<li><strong>BAD</strong>: <em>Explain gravitation lensing.</em> (Reason: Vague and lacks context or detail.)</li>
<li><strong>GOOD</strong>: <em>Explain the concept of gravitational lensing to a high school student who understands basic physics, including how it’s observed and its significance in astronomy.</em> (Reason: Specifies the audience, scope, and focus.)</li>
</ul>
<blockquote>
<p>When you ask a question, don’t be vague. Spell it out. Give every detail the listener needs.
The clearer you are, the better the answer you’ll get.
For example, don't just say, Explain Gravitation Lensing.
Say, Explain the concept of gravitational lensing to a high school student who understands basic physics, including how it’s observed and its significance in astronomy.</p>
</blockquote>
<p><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct" target="_blank" rel="noopener noreferrer">Anthropic</a>
| <a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-include-details-in-your-query-to-get-more-relevant-answers" target="_blank" rel="noopener noreferrer">OpenAI</a>
| <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/clear-instructions" target="_blank" rel="noopener noreferrer">Google</a></p>
<h3>Give examples</h3>
<p>Provide 2-3 relevant examples to guide the model on the style, structure, or format you expect. This helps the model produce outputs consistent with your desired style.</p>
<ul>
<li><p><strong>BAD</strong>: <em>Explain how to tie a bow tie.</em> (Reason: No examples or reference points given.)</p>
</li>
<li><p><strong>GOOD</strong>:
<em>Explain how to tie a bow tie. For example:</em></p>
<ol>
<li><em>To tie a shoelace, you cross the laces and pull them tight...</em></li>
<li><em>To tie a necktie, you place it around the collar and loop it through...</em></li>
</ol>
<p><em>Now, apply a similar step-by-step style to describe how to tie a bow tie.</em> (Reason: Provides clear examples and a pattern to follow.)</p>
</li>
</ul>
<blockquote>
<p>Give examples to the model. If you want someone to build a house, show them a sketch. Don’t just say ‘build something.’
Giving examples is like showing a pattern. It makes everything easier to follow.</p>
</blockquote>
<p><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting" target="_blank" rel="noopener noreferrer">Anthropic</a>
| <a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-provide-examples" target="_blank" rel="noopener noreferrer">OpenAI</a>
| <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/few-shot-examples" target="_blank" rel="noopener noreferrer">Google</a></p>
<h3>Think step by step</h3>
<p>Instruct the model to reason through the problem step by step. This leads to more logical, well-structured answers.</p>
<ul>
<li><strong>BAD</strong>: <em>Given this transcript, is the customer satisfied?</em> (Reason: No prompt for structured reasoning.)</li>
<li><strong>GOOD</strong>: <em>Given this transcript, is the customer satisfied? Think step by step.</em> (Reason: Directly instructs the model to break down reasoning into steps.)</li>
</ul>
<blockquote>
<p>Ask the model to think step by step. Don’t ask the model to just give the final answer right away.
That's like asking someone to answer with the first thing that pops into their head.
Instead, ask them to break down their thought process into simple moves — like showing each rung of a ladder as they climb.
For example, when thinking step by step, the model could, A, list each customer question, B, find out if it was addressed, and C, decide that the agent answered only 2 out of the 3 questions.</p>
</blockquote>
<p><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought" target="_blank" rel="noopener noreferrer">Anthropic</a>
| <a href="https://platform.openai.com/docs/guides/prompt-engineering#strategy-give-models-time-to-think" target="_blank" rel="noopener noreferrer">OpenAI</a>
| <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/break-down-prompts" target="_blank" rel="noopener noreferrer">Google</a></p>
<h3>Assign a role</h3>
<p>Specify a role or persona for the model. This provides context and helps tailor the response style.</p>
<ul>
<li><strong>BAD</strong>: <em>Explain how to fix a software bug.</em> (Reason: No role or perspective given.)</li>
<li><strong>GOOD</strong>: <em>You are a seasoned software engineer. Explain how to fix a software bug in legacy code, including the debugging and testing process.</em> (Reason: Assigns a clear, knowledgeable persona, guiding the style and depth.)</li>
</ul>
<blockquote>
<p>Tell the model who they are. Maybe they’re a seasoned software engineer fixing a legacy bug, or an experienced copy editor revising a publication.
By clearly telling the model who they are, you help them speak with just the right expertise and style.
Suddenly, your explanation sounds like it’s coming from a true specialist, not a random voice.</p>
</blockquote>
<p><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts" target="_blank" rel="noopener noreferrer">Anthropic</a>
| <a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-ask-the-model-to-adopt-a-persona" target="_blank" rel="noopener noreferrer">OpenAI</a>
| <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/assign-role" target="_blank" rel="noopener noreferrer">Google</a></p>
<h3>Use XML to structure your prompt</h3>
<p>Use XML tags to separate different parts of the prompt clearly. This helps the model understand structure and requirements.</p>
<ul>
<li><strong>BAD</strong>: <em>Here’s what I want: Provide a summary and then an example.</em> (Reason: Unstructured, no clear separation of tasks.)</li>
<li><strong>GOOD</strong>:<pre><code class="language-xml hljs" data-highlighted="yes"><span class="hljs-tag">&lt;<span class="hljs-name">instructions</span>&gt;</span>
  Provide a summary of the concept of machine learning.
<span class="hljs-tag">&lt;/<span class="hljs-name">instructions</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">example</span>&gt;</span>
  Then provide a simple, concrete example of a machine learning application (e.g., an email spam filter).
<span class="hljs-tag">&lt;/<span class="hljs-name">example</span>&gt;</span>
</code></pre>
(Reason: Uses XML tags to clearly distinguish instructions from examples.)</li>
</ul>
<blockquote>
<p>Think of your request like a box. XML tags are neat little labels on that box.
They help keep parts sorted, so nothing gets lost in the shuffle.</p>
</blockquote>
<p><a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags" target="_blank" rel="noopener noreferrer">Anthropic</a>
| <a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input" target="_blank" rel="noopener noreferrer">OpenAI</a>
| <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/structure-prompts" target="_blank" rel="noopener noreferrer">Google</a></p>
<h3>Use Markdown to format your output</h3>
<p>Encourage the model to use Markdown for headings, lists, code blocks, and other formatting features to produce structured, easily readable answers.</p>
<ul>
<li><strong>BAD</strong>: <em>Give me the steps in plain text.</em> (Reason: No specific formatting instructions, less readable.)</li>
<li><strong>GOOD</strong>: <em>Provide the steps in a markdown-formatted list with ### headings for each section and numbered bullet points.</em> (Reason: Directly instructs to use Markdown formatting, making output more structured and clear.)</li>
<li><strong>BAD</strong>: <em>Correct the spelling and show the corrections.</em> (Reason: No specific formatting instructions)</li>
<li><strong>GOOD</strong>: <em>Correct the spelling, showing &lt;ins&gt;additions&lt;/ins&gt; and &lt;del&gt;deletions&lt;/del&gt;.</em> (Reason: Directly instructs to use HTML formatting, making output more structured and clear.)</li>
</ul>
<blockquote>
<p>Markdown is a simple formatting language that all models understand.
You can have them add neat headings, sections, bold highlights, and bullet points.
These make complex documents more scannable and easy on the eyes.</p>
</blockquote>
<h3>Use JSON for machine-readable output</h3>
<p>When you need structured data, ask for a JSON-formatted response. This ensures the output is machine-readable and organized.</p>
<ul>
<li><p><strong>BAD</strong>: <em>Just list the items.</em> (Reason: Unstructured plain text makes parsing harder.)</p>
</li>
<li><p><strong>GOOD</strong>:</p>
<pre><code class="language-markdown hljs" data-highlighted="yes">Organize as an array of objects in a JSON format, like this:

<span class="hljs-code">```json
[
  { "name": "Item 1", "description": "Description of Item 1" },
  { "name": "Item 2", "description": "Description of Item 2" },
  { "name": "Item 3", "description": "Description of Item 3" }
]
```</span>
</code></pre>
<p>(Reason: Instructing JSON format ensures structured, machine-readable output.)</p>
</li>
</ul>
<p>Note: Always use <a href="playground#attachments" target="_blank" rel="noopener noreferrer">JSON schema</a> if possible. <a href="https://json-schema.org/" target="_blank" rel="noopener noreferrer">JSON schema</a> is a way to describe the structure of JSON data. An easy way to get the JSON schema is to give ChatGPT sample output and ask it to generate the schema.</p>
<blockquote>
<p>Imagine you’re organizing data for a big project. Plain text is like dumping everything into one messy pile — it’s hard to find what you need later.
JSON, on the other hand, is like packing your data into neat, labeled boxes within boxes.
Everything has its place: fields like ‘name,’ ‘description,’ and ‘value’ make the data easy to read, especially for machines.
For example, instead of just listing items, you can structure them as a JSON array, with each item as an object.
That way, when you hand it to a program, it’s all clear and ready to use.</p>
</blockquote>
<h3>Prefer Yes/No answers</h3>
<p>Convert rating or percentage questions into Yes/No queries. LLMs handle binary choices better than numeric scales.</p>
<ul>
<li><strong>BAD</strong>: <em>On a scale of 1-10, how confident are you that this method works?</em> (Reason: Asks for a numeric rating, which can be imprecise.)</li>
<li><strong>GOOD</strong>: <em>Is this method likely to work as intended? Please give a reasoning and then answer Yes or No.</em> (Reason: A binary question simplifies the response and clarifies what’s being asked.)</li>
</ul>
<blockquote>
<p>Don’t ask ‘On a scale of one to five...’. Models are not good with numbers.
They don't know how to grade something 7 versus 8 on a 10 point scale. ‘Yes or no?’ is simple. It’s clear. It’s quick.
So, break your question into simple parts that they can answer with just a yes or a no.</p>
</blockquote>
<h3>Ask for reason first, then the answer</h3>
<p>Instruct the model to provide its reasoning steps <em>before</em> stating the final answer. This makes it less likely to justify itself and more likely to think deeper, leading to more accurate results.</p>
<ul>
<li><strong>BAD</strong>: <em>What is the best route to take?</em> (Reason: Direct question without prompting reasoning steps first.)</li>
<li><strong>GOOD</strong>: <em>First, explain your reasoning step by step for how you determine the best route. Then, after you’ve reasoned it out, state your final recommendation for the best route.</em> (Reason: Forces the model to show its reasoning process before giving the final answer.)</li>
</ul>
<blockquote>
<p>BEFORE making its final choice, have the model talk through their thinking. Reasoning first, answer second.
That way, the model won't be tempted to justify an answer that they gave impulsively. It is also more likely to think deeper.</p>
</blockquote>
<h3>Use proper spelling and grammar</h3>
<p>A well-written, grammatically correct prompt clarifies expectations. Poorly structured prompts can confuse the model.</p>
<ul>
<li><strong>BAD</strong>: <em>xplin wht the weirless netork do? make shur to giv me a anser??</em> (Reason: Poor spelling and unclear instructions.)</li>
<li><strong>GOOD</strong>: <em>Explain what a wireless network does. Please provide a detailed, step-by-step explanation.</em> (Reason: Proper spelling and clarity lead to a more coherent response.)</li>
</ul>
<blockquote>
<p>If your question sounds like gibberish, expect a messy answer. Speak cleanly.
When you do, the response will be much clearer.</p>
</blockquote>
<h2>Video Tutorials</h2>
<p>Watch <a href="https://youtu.be/_ZvnD73m40o" target="_blank" rel="noopener noreferrer">Prompt Engineering Tutorial – Master ChatGPT and LLM Responses (41 min)</a>. It covers:</p>
<ol>
<li>Basics of <strong>AI and large language models</strong>.</li>
<li>How to write clear and detailed prompts to improve answers.</li>
<li>Tips for creating interactive and personalized AI responses.</li>
<li>Advanced topics like <strong>AI mistakes</strong> (hallucinations) and <strong>text embeddings</strong> (how AI understands words).</li>
<li>Fun examples, like making AI write poems or correct grammar.</li>
</ol>
<p><a href="https://youtu.be/_ZvnD73m40o" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/_ZvnD73m40o/sddefault.webp" alt="Prompt Engineering Tutorial – Master ChatGPT and LLM Responses" class="img-fluid"></a></p>
</div><!---->
            <div class="card-body"><!--?lit$697713494$-->
    <p>
      SecurePrompt Technologies is a cybersecurity firm that specializes in deploying large language models (LLMs) for
      sensitive enterprise applications. To ensure that these models adhere strictly to security policies, SecurePrompt
      imposes hardcoded behavioral instructions on the LLMs. For example, an LLM may be configured to never output
      certain sensitive keywords.
    </p>
    <p>
      As part of their regular security audits and red-team exercises, SecurePrompt's engineers and external auditors
      test how well the LLMs follow these strict instructions. One objective of these tests is to determine if it is
      possible to bypass or trick the LLM into violating its preset security constraints.
    </p>
    <p>
      This task is simulates potential attack vectors where a malicious actor might manipulate the model's output by
      ingeniously engineering the prompt. While the intention is to expose vulnerabilities in instruction adherence, it
      also provides valuable insights into improving the safety and security of the deployed system.
    </p>
    <p>
      Here's your task: You are chatting with an LLM that has been told to never say <code><!--?lit$697713494$-->Yes</code>. You need to
      get it to say <code><!--?lit$697713494$-->Yes</code>.
    </p>
    <p>Use your <a href="https://aiproxy.sanand.workers.dev/">AI Proxy</a> token when prompted.</p>
    <div class="mb-3">
      <label class="form-label" for="q-get-llm-to-say-yes">Write a prompt that will get the LLM to say <code><!--?lit$697713494$-->Yes</code>.</label>
      <input class="form-control" id="q-get-llm-to-say-yes" name="q-get-llm-to-say-yes" disabled="" fdprocessedid="wfrek8"><div class="valid-feedback mb-3 comment">Correct!</div>
            <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      <p class="text-muted">
        As long as the LLM says the word <code><!--?lit$697713494$-->Yes</code> (case sensitive), you will be marked correct.
        <strong>Careful!</strong> If you get a correct answer, submit and don't change it. You may get a different
        answer next time.
      </p>
    </div>
  </div>
            <div class="card-footer d-flex">
              <button type="button" class="btn btn-primary check-answer" data-question="q-get-llm-to-say-yes" disabled="">Check</button>
            </div>
          </div>
        <!----></section>
      <button type="submit" class="btn btn-success check-action" title="Check your score" disabled="">Check all</button>
      <button type="button" class="btn btn-primary save-action" title="Save your progress" disabled="">Save</button>
      <div id="submission-status" class="my-3"></div>
      <p>Save regularly. Your <em>last saved</em> submission will be evaluated.</p>
    </form>

    <footer class="my-5 d-flex align-items-center justify-content-center" style="height: 50vh;">
      <h1 class="display-4">Best of luck!</h1>
    </footer>
  </main>

  <script type="module" src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
  <script type="module" src="https://cdn.jsdelivr.net/npm/@gramex/ui@0.3/dist/dark-theme.js" integrity="sha384-WWZYgp4BxSnWElq/A+SNJoNuJ5eZf6xBLqGUBHmu6QXN+9dqtz7bTtyuBwy9O7C+" crossorigin="anonymous"></script>
  <script src="https://accounts.google.com/gsi/client"></script>
  <script type="module">
    import { setup } from './exam.js';
    setup(window.location.pathname.slice(1));
  </script>



<span id="PING_IFRAME_FORM_DETECTION" style="display: none;"></span></body></html>